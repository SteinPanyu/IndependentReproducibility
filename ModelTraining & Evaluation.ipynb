{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = GroupKFold(n_splits=n_splits \n",
    ")\n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    22\n",
       "0    20\n",
       "2    13\n",
       "4    13\n",
       "1     9\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_user['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, groups, t, datetimes = load(p)\n",
    "cluster_label = similar_user['cluster'].value_counts().index[0]\n",
    "similar_users_in_cluster_0 = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "# Check if each value in 'groups' is in 'similar_users_in_cluster_0'\n",
    "mask = np.isin(groups, similar_users_in_cluster_0)\n",
    "\n",
    "# Filter 'groups' based on the mask\n",
    "filtered_groups = groups[mask]\n",
    "# Filter 'X' and 'y' based on the mask\n",
    "X_filtered = X[mask]\n",
    "y_filtered = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 16:53:47,038\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] Success: [dummy#5] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] Success: [dummy#1] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] Success: [dummy#3] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] Success: [dummy#3] Training (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] Success: [dummy#5] Training (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] Success: [dummy#1] Training (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] Success: [dummy#2] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] Success: [dummy#2] Training (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] Success: [dummy#4] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] In progress: [dummy#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] Success: [dummy#4] Training (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#3] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#4] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#2] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#1] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#3] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#4] 1-th Feature selection (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#5] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#5] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#2] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m - # Sel. Feat.: 1 (# Cat. = 1; # Num. = 0)\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#2] Training (0.09s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#1] 1-th Feature selection (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m - # Sel. Feat.: 9 (# Cat. = 0; # Num. = 9)\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] In progress: [rf_ns#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#3] Training (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#4] Training (0.14s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#5] Training (0.14s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:48] Success: [rf_ns#1] Training (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#3] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#3] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#3] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#3] Oversampling (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#2] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#2] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m - # Sel. Feat.: 1 (# Cat. = 1; # Num. = 0)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#2] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#1] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#1] 1-th Feature selection (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m - # Sel. Feat.: 9 (# Cat. = 0; # Num. = 9)\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#1] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#1] Oversampling (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#5] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#5] 1-th Feature selection (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#5] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#5] Oversampling (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#4] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#4] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#4] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#4] Oversampling (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#4] Training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#3] Training (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] Failure: [rf_os#2] Oversampling (0.06s). Keep running without this task. Caused by: \n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m   File \"/tmp/ipykernel_532788/515145000.py\", line 95, in _log\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m   File \"/tmp/ipykernel_532788/515145000.py\", line 161, in _train\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m   File \"/home/panyu/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/base.py\", line 203, in fit_resample\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m     return super().fit_resample(X, y)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m   File \"/home/panyu/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/base.py\", line 88, in fit_resample\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m     output = self._fit_resample(X, y)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m   File \"/home/panyu/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/over_sampling/_smote/base.py\", line 580, in _fit_resample\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m     self._validate_estimator()\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m   File \"/home/panyu/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/over_sampling/_smote/base.py\", line 564, in _validate_estimator\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m ValueError: SMOTE-NC is not designed to work only with categorical features. It requires some numerical features.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [rf_os#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#1] Training (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#5] Training (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#4] Training (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] Success: [rf_os#2] Training (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#1] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#2] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#3] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#4] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#4] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#4] Training (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#1] 1-th Feature selection (0.06s).\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m - # Sel. Feat.: 9 (# Cat. = 0; # Num. = 9)\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#1] Training (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#5] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#5] 1-th Feature selection (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#5] Training (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#2] 1-th Feature selection (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m - # Sel. Feat.: 1 (# Cat. = 1; # Num. = 0)\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#2] Training (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#3] 1-th Feature selection (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m - # Orig. Feat.: 3412 (# Cat. = 58; # Num. = 3354)\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m - # Sel. Feat.: 8 (# Cat. = 0; # Num. = 8)\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_ns#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [xgb_ns#3] Training (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] Success: [xgb_os#2] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542711)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542714)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] Success: [xgb_os#1] Normalizing numeric features (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=542710)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=542709)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] Success: [xgb_os#3] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=542712)\u001b[0m [23-05-19 16:53:49] In progress: [xgb_os#3] 1-th Feature selection.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    threshold=1e-5\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_DUMMY),\n",
    "        oversample=False,\n",
    "        select=None,\n",
    "        name='dummy'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    )\n",
    "]\n",
    "\n",
    "with on_ray(num_cpus=6):\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):\n",
    "        p = os.path.join(PATH_INTERMEDIATE, 'feat',f'{l}.pkl')\n",
    "        par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', l)\n",
    "        os.makedirs(par_dir, exist_ok=True)\n",
    "        \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        #The following code is for similar-user model\n",
    "        ###########################################\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[4]\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "        ###########################################\n",
    "        \n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] \n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast],axis=1)\n",
    "        feat_final = pd.concat([feat_baseline, feat_sleep, feat_today_sensor],axis=1)\n",
    "        X = feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups,\n",
    "            path=par_dir,\n",
    "            categories=cats,\n",
    "            normalize=True,\n",
    "            split='groupk',\n",
    "            split_params= {'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_prob,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            ) if not is_same_y else np.nan\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_prob[:, 1], average=None\n",
    "        ) if not is_same_y else np.nan\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_prob[:, i], pos_label=c, average=None\n",
    "            ) \n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_prob, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_prob=y_prob[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>split</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_class_ratio</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>...</th>\n",
       "      <th>train_pre_1</th>\n",
       "      <th>train_rec_1</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_roauc</th>\n",
       "      <th>train_prauc_0</th>\n",
       "      <th>train_prauc_ref_0</th>\n",
       "      <th>train_prauc_1</th>\n",
       "      <th>train_prauc_ref_1</th>\n",
       "      <th>train_log_loss</th>\n",
       "      <th>train_brier_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>73</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>2.318182</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.876483</td>\n",
       "      <td>0.860273</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.885587</td>\n",
       "      <td>0.541284</td>\n",
       "      <td>0.508894</td>\n",
       "      <td>0.163952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>2.235294</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.757202</td>\n",
       "      <td>0.830096</td>\n",
       "      <td>0.787548</td>\n",
       "      <td>0.478814</td>\n",
       "      <td>0.847537</td>\n",
       "      <td>0.521186</td>\n",
       "      <td>0.526263</td>\n",
       "      <td>0.174373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>rf_os</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>2.235294</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792308</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.814229</td>\n",
       "      <td>0.903364</td>\n",
       "      <td>0.900263</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.906807</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.392252</td>\n",
       "      <td>0.128268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517695</td>\n",
       "      <td>0.622541</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.399546</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.663807</td>\n",
       "      <td>0.235479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_os</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>1.238095</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785124</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.855616</td>\n",
       "      <td>0.837850</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.859670</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.554570</td>\n",
       "      <td>0.183349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label     alg split  n_feature  test_inst  test_inst_0  test_inst_1  \\\n",
       "0  stress  xgb_ns     1          9         73           51           22   \n",
       "1  stress  xgb_ns     3          8         55           38           17   \n",
       "2  stress   rf_os     3          8         55           38           17   \n",
       "3  stress  xgb_ns     2          1         63           11           52   \n",
       "4  stress  xgb_os     5          8         47           26           21   \n",
       "\n",
       "   test_class_ratio  test_true_0_pred_0  test_true_0_pred_1  ...  train_pre_1  \\\n",
       "0          2.318182                   8                  43  ...     0.817391   \n",
       "1          2.235294                   9                  29  ...     0.766667   \n",
       "2          2.235294                  10                  28  ...     0.792308   \n",
       "3          0.211538                  11                   0  ...     0.000000   \n",
       "4          1.238095                   8                  18  ...     0.785124   \n",
       "\n",
       "   train_rec_1  train_f1_1  train_roauc  train_prauc_0  train_prauc_ref_0  \\\n",
       "0     0.796610    0.806867     0.876483       0.860273           0.458716   \n",
       "1     0.747967    0.757202     0.830096       0.787548           0.478814   \n",
       "2     0.837398    0.814229     0.903364       0.900263           0.500000   \n",
       "3     0.000000    0.000000     0.517695       0.622541           0.614035   \n",
       "4     0.760000    0.772358     0.855616       0.837850           0.500000   \n",
       "\n",
       "   train_prauc_1  train_prauc_ref_1  train_log_loss  train_brier_loss  \n",
       "0       0.885587           0.541284        0.508894          0.163952  \n",
       "1       0.847537           0.521186        0.526263          0.174373  \n",
       "2       0.906807           0.500000        0.392252          0.128268  \n",
       "3       0.399546           0.385965        0.663807          0.235479  \n",
       "4       0.859670           0.500000        0.554570          0.183349  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "#for l in ['valence', 'arousal', 'disturbance', 'stress']:\n",
    "for l in [ 'stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        if f == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        model, pid = f[:f.index('.pkl')].split('#')\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "        X, y = res.X_test, res.y_test\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_test = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        X, y = res.X_train, res.y_train\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_train = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        RESULTS_EVAL.append({\n",
    "            'label': l,\n",
    "            'alg': model,\n",
    "            'split': pid,\n",
    "            'n_feature': len(X.columns),\n",
    "            **{\n",
    "                f'test_{k}': v for k, v in ev_test.items()\n",
    "            },\n",
    "            **{\n",
    "                f'train_{k}': v for k, v in ev_train.items()\n",
    "            }\n",
    "        })\n",
    "    \n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>metric</th>\n",
       "      <th>n</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>value_count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>med</th>\n",
       "      <th>range</th>\n",
       "      <th>conf.</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>split</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2:1, 3:1, 5:1, 4:1, 1:1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>n_feature</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17060.0</td>\n",
       "      <td>3412.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3412.0</td>\n",
       "      <td>(3412, 3412)</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>291.0</td>\n",
       "      <td>58.2</td>\n",
       "      <td>10.059821</td>\n",
       "      <td>55.0</td>\n",
       "      <td>(47, 73)</td>\n",
       "      <td>(45.70908234656642, 70.69091765343359)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst_0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151.0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>15.056560</td>\n",
       "      <td>26.0</td>\n",
       "      <td>(11, 51)</td>\n",
       "      <td>(11.504811471357215, 48.895188528642784)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst_1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.982131</td>\n",
       "      <td>22.0</td>\n",
       "      <td>(17, 52)</td>\n",
       "      <td>(10.638890755279679, 45.361109244720325)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label    alg       metric  n  cardinality              value_count  \\\n",
       "0  stress  dummy        split  5          5.0  2:1, 3:1, 5:1, 4:1, 1:1   \n",
       "1  stress  dummy    n_feature  5          NaN                      NaN   \n",
       "2  stress  dummy    test_inst  5          NaN                      NaN   \n",
       "3  stress  dummy  test_inst_0  5          NaN                      NaN   \n",
       "4  stress  dummy  test_inst_1  5          NaN                      NaN   \n",
       "\n",
       "       sum    mean         SD     med         range  \\\n",
       "0      NaN     NaN        NaN     NaN           NaN   \n",
       "1  17060.0  3412.0   0.000000  3412.0  (3412, 3412)   \n",
       "2    291.0    58.2  10.059821    55.0      (47, 73)   \n",
       "3    151.0    30.2  15.056560    26.0      (11, 51)   \n",
       "4    140.0    28.0  13.982131    22.0      (17, 52)   \n",
       "\n",
       "                                      conf.  nan_count  \n",
       "0                                       NaN        NaN  \n",
       "1                                (nan, nan)        0.0  \n",
       "2    (45.70908234656642, 70.69091765343359)        0.0  \n",
       "3  (11.504811471357215, 48.895188528642784)        0.0  \n",
       "4  (10.638890755279679, 45.361109244720325)        0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "    ['label', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                alg=row.alg,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"13\" halign=\"left\">mean_sd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_f1_0</th>\n",
       "      <th>test_f1_1</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>train_class_ratio</th>\n",
       "      <th>train_f1_0</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_f1_macro</th>\n",
       "      <th>train_inst_0</th>\n",
       "      <th>train_inst_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">stress</th>\n",
       "      <th>dummy</th>\n",
       "      <td>3412.0 (0.0)</td>\n",
       "      <td>0.362 (0.15)</td>\n",
       "      <td>0.33 (0.34)</td>\n",
       "      <td>0.187 (0.256)</td>\n",
       "      <td>0.259 (0.082)</td>\n",
       "      <td>30.2 (15.057)</td>\n",
       "      <td>28.0 (13.982)</td>\n",
       "      <td>1.106 (0.292)</td>\n",
       "      <td>0.426 (0.39)</td>\n",
       "      <td>0.278 (0.38)</td>\n",
       "      <td>0.352 (0.017)</td>\n",
       "      <td>120.8 (15.057)</td>\n",
       "      <td>112.0 (13.982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_ns</th>\n",
       "      <td>6.8 (3.271)</td>\n",
       "      <td>0.344 (0.105)</td>\n",
       "      <td>0.302 (0.159)</td>\n",
       "      <td>0.352 (0.155)</td>\n",
       "      <td>0.327 (0.103)</td>\n",
       "      <td>30.2 (15.057)</td>\n",
       "      <td>28.0 (13.982)</td>\n",
       "      <td>1.106 (0.292)</td>\n",
       "      <td>0.862 (0.088)</td>\n",
       "      <td>0.741 (0.36)</td>\n",
       "      <td>0.801 (0.215)</td>\n",
       "      <td>120.8 (15.057)</td>\n",
       "      <td>112.0 (13.982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_os</th>\n",
       "      <td>6.8 (3.271)</td>\n",
       "      <td>0.364 (0.103)</td>\n",
       "      <td>0.327 (0.129)</td>\n",
       "      <td>0.374 (0.159)</td>\n",
       "      <td>0.351 (0.102)</td>\n",
       "      <td>30.2 (15.057)</td>\n",
       "      <td>28.0 (13.982)</td>\n",
       "      <td>1.118 (0.264)</td>\n",
       "      <td>0.869 (0.086)</td>\n",
       "      <td>0.742 (0.361)</td>\n",
       "      <td>0.806 (0.217)</td>\n",
       "      <td>126.4 (8.204)</td>\n",
       "      <td>116.0 (15.953)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_ns</th>\n",
       "      <td>6.8 (3.271)</td>\n",
       "      <td>0.377 (0.129)</td>\n",
       "      <td>0.372 (0.108)</td>\n",
       "      <td>0.366 (0.209)</td>\n",
       "      <td>0.369 (0.14)</td>\n",
       "      <td>30.2 (15.057)</td>\n",
       "      <td>28.0 (13.982)</td>\n",
       "      <td>1.106 (0.292)</td>\n",
       "      <td>0.79 (0.059)</td>\n",
       "      <td>0.641 (0.361)</td>\n",
       "      <td>0.716 (0.194)</td>\n",
       "      <td>120.8 (15.057)</td>\n",
       "      <td>112.0 (13.982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_os</th>\n",
       "      <td>6.8 (3.271)</td>\n",
       "      <td>0.363 (0.114)</td>\n",
       "      <td>0.364 (0.152)</td>\n",
       "      <td>0.324 (0.199)</td>\n",
       "      <td>0.344 (0.123)</td>\n",
       "      <td>30.2 (15.057)</td>\n",
       "      <td>28.0 (13.982)</td>\n",
       "      <td>1.118 (0.264)</td>\n",
       "      <td>0.803 (0.058)</td>\n",
       "      <td>0.642 (0.364)</td>\n",
       "      <td>0.723 (0.199)</td>\n",
       "      <td>126.4 (8.204)</td>\n",
       "      <td>116.0 (15.953)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean_sd                                               \\\n",
       "metric            n_feature       test_acc      test_f1_0      test_f1_1   \n",
       "label  alg                                                                 \n",
       "stress dummy   3412.0 (0.0)   0.362 (0.15)    0.33 (0.34)  0.187 (0.256)   \n",
       "       rf_ns    6.8 (3.271)  0.344 (0.105)  0.302 (0.159)  0.352 (0.155)   \n",
       "       rf_os    6.8 (3.271)  0.364 (0.103)  0.327 (0.129)  0.374 (0.159)   \n",
       "       xgb_ns   6.8 (3.271)  0.377 (0.129)  0.372 (0.108)  0.366 (0.209)   \n",
       "       xgb_os   6.8 (3.271)  0.363 (0.114)  0.364 (0.152)  0.324 (0.199)   \n",
       "\n",
       "                                                                              \\\n",
       "metric         test_f1_macro    test_inst_0    test_inst_1 train_class_ratio   \n",
       "label  alg                                                                     \n",
       "stress dummy   0.259 (0.082)  30.2 (15.057)  28.0 (13.982)     1.106 (0.292)   \n",
       "       rf_ns   0.327 (0.103)  30.2 (15.057)  28.0 (13.982)     1.106 (0.292)   \n",
       "       rf_os   0.351 (0.102)  30.2 (15.057)  28.0 (13.982)     1.118 (0.264)   \n",
       "       xgb_ns   0.369 (0.14)  30.2 (15.057)  28.0 (13.982)     1.106 (0.292)   \n",
       "       xgb_os  0.344 (0.123)  30.2 (15.057)  28.0 (13.982)     1.118 (0.264)   \n",
       "\n",
       "                                                                            \\\n",
       "metric            train_f1_0     train_f1_1 train_f1_macro    train_inst_0   \n",
       "label  alg                                                                   \n",
       "stress dummy    0.426 (0.39)   0.278 (0.38)  0.352 (0.017)  120.8 (15.057)   \n",
       "       rf_ns   0.862 (0.088)   0.741 (0.36)  0.801 (0.215)  120.8 (15.057)   \n",
       "       rf_os   0.869 (0.086)  0.742 (0.361)  0.806 (0.217)   126.4 (8.204)   \n",
       "       xgb_ns   0.79 (0.059)  0.641 (0.361)  0.716 (0.194)  120.8 (15.057)   \n",
       "       xgb_os  0.803 (0.058)  0.642 (0.364)  0.723 (0.199)   126.4 (8.204)   \n",
       "\n",
       "                               \n",
       "metric           train_inst_1  \n",
       "label  alg                     \n",
       "stress dummy   112.0 (13.982)  \n",
       "       rf_ns   112.0 (13.982)  \n",
       "       rf_os   116.0 (15.953)  \n",
       "       xgb_ns  112.0 (13.982)  \n",
       "       xgb_os  116.0 (15.953)  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "        ['n_feature', 'train_class_ratio', 'train_inst_0', 'train_inst_1', 'test_inst_0', 'test_inst_1', 'test_acc', 'test_f1_0' ,'test_f1_1', 'test_f1_macro', 'train_f1_0' ,'train_f1_1', 'train_f1_macro',]\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep='')\n",
    ").pivot(\n",
    "    index=['label', 'alg'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if hasattr(estimator, 'model'):\n",
    "        estimator = estimator.model\n",
    "    \n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "\n",
    "for f in os.listdir(DIR_EVAL):\n",
    "    res = load(os.path.join(DIR_EVAL, f))\n",
    "    \n",
    "    f_norm = f[:f.index('.pkl')]\n",
    "    cv = f_norm[:f.rindex('#')]\n",
    "    \n",
    "    feat_imp = feature_importance(res.estimator)\n",
    "    if not feat_imp:\n",
    "        continue\n",
    "        \n",
    "    names, importance = feat_imp\n",
    "    d = pd.DataFrame(\n",
    "        importance.reshape(1, -1),\n",
    "        columns=names\n",
    "    )\n",
    "    IMPORTANCE_EVAL[cv].append(d)\n",
    "\n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for k, v in IMPORTANCE_EVAL.items():\n",
    "    l, o, a = k.split('#')\n",
    "    \n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        oversampling=o,\n",
    "        alg=a\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0)\n",
    "IMPORTANCE_SUMMARY.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 22 -u cm\n",
    "\n",
    "plots <- list()\n",
    "\n",
    "#for (l in c('valence', 'arousal', 'stress', 'disturbance')) {\n",
    "for (l in c( 'stress')) {\n",
    "    data <- IMPORTANCE_SUMMARY %>% filter(\n",
    "        (label == l) & (oversampling == 'os')\n",
    "    )\n",
    "\n",
    "    p_label <- ggplot() + geom_text(\n",
    "        aes(x=.5, y=.5),\n",
    "        label=str_to_title(l), \n",
    "        family='ssp', \n",
    "        fontface='bold',\n",
    "        size=4\n",
    "    ) + theme_void()\n",
    "\n",
    "    data_rf <- data %>% filter(\n",
    "        alg == 'rf'\n",
    "    )\n",
    "\n",
    "    p_rf <- ggplot(\n",
    "        data %>% filter(alg == 'rf') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='Random Forest'\n",
    "    ) + ylim(\n",
    "        c(0, 0.08)\n",
    "    )\n",
    "    p_xgb <- ggplot(\n",
    "        data %>% filter(alg == 'xgb') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='XGBoost'\n",
    "    ) + ylim(\n",
    "        c(0, 0.08)\n",
    "    )\n",
    "    plots[[paste(l, 'label', sep='_')]] <- p_label\n",
    "    plots[[paste(l, 'rf', sep='_')]] <- p_rf\n",
    "    plots[[paste(l, 'xgb', sep='_')]] <- p_xgb\n",
    "}\n",
    "\n",
    "#p <- plots$arousal_label + plots$valence_label\n",
    "#p <- p / (plots$arousal_rf | plots$arousal_xgb | plots$valence_rf | plots$valence_xgb)\n",
    "#p <- p / (plots$stress_label + plots$disturbance_label)\n",
    "#p <- p / (plots$stress_rf | plots$stress_xgb | plots$disturbance_rf | plots$disturbance_xgb)\n",
    "p <- plots$stress_label \n",
    "p <- p / (plots$stress_rf | plots$stress_xgb)\n",
    "\n",
    "\n",
    "p <- p + plot_layout(\n",
    "    heights=c(1.1, 10, 1.1, 10)\n",
    ")\n",
    "\n",
    "ggsave(paste('./fig/imp.pdf'), plot=p, width=26, height=20, unit='cm', device=cairo_pdf)\n",
    "print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
