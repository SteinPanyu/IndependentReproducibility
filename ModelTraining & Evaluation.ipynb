{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_low_variance(agg_feature, threshold=.0000001):\n",
    "    agg_feature_non_zero_var = agg_feature.loc[:,agg_feature.var()>threshold]\n",
    "    num_removed = agg_feature.shape[1]-agg_feature_non_zero_var.shape[1]\n",
    "    print(f'{num_removed}/{agg_feature.shape[1]} features with variance < {threshold} removed')\n",
    "    return agg_feature_non_zero_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def remove_pairwise_corr(agg_feature_percent_missing, PAIRWISE_CORR_THRESHOLD=0.8, outcome_variable=None):\n",
    "    if outcome_variable is not None:\n",
    "        outcome_variable = pd.Series(outcome_variable, index=agg_feature_percent_missing.index, name=\"outcome\")\n",
    "        corr_with_outcome = pd.merge(outcome_variable, agg_feature_percent_missing, left_index=True, right_index=True).corr()[outcome_variable.name].abs().sort_values(ascending=False)\n",
    "        importance_order = corr_with_outcome.index[1:].tolist()\n",
    "        agg_feature_percent_missing = agg_feature_percent_missing[importance_order]\n",
    "\n",
    "    Matrix = agg_feature_percent_missing.corr().abs()\n",
    "    \n",
    "    upper_triangle = Matrix.where(np.triu(np.ones(Matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    correlated_features = set()\n",
    "    for feature in upper_triangle.columns:\n",
    "        highly_correlated = upper_triangle[feature][upper_triangle[feature] > PAIRWISE_CORR_THRESHOLD].index\n",
    "        correlated_features.update(highly_correlated)\n",
    "\n",
    "    kept_features = list(set(agg_feature_percent_missing.columns) - correlated_features)\n",
    "    print(f\"Pairwise Corr: kept only {len(kept_features)}/{len(agg_feature_percent_missing.columns)} features\")\n",
    "    return agg_feature_percent_missing[kept_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "class CustomCV:\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y, groups):\n",
    "        logo = LeaveOneGroupOut()\n",
    "\n",
    "        for train_users, test_users in logo.split(X, y, groups):\n",
    "            X_train_users, X_test_user = X.loc[train_users], X.loc[test_users]\n",
    "            y_train_users, y_test_user = y[train_users], y[test_users]\n",
    "            group_train_users, group_test_user = groups[train_users], groups[test_users]\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=self.n_splits) \n",
    "\n",
    "            for train_index, test_index in tscv.split(X_test_user):\n",
    "                X_train, X_test = pd.concat([X_train_users, X_test_user.iloc[train_index]]), X_test_user.iloc[test_index]\n",
    "                y_train, y_test = np.concatenate([y_train_users, y_test_user[train_index]]), y_test_user[test_index]\n",
    "\n",
    "                yield (X_train.index, X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "    datetimes_train: np.ndarray = None\n",
    "    datetimes_test: np.ndarray = None\n",
    "\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    elif alg == 'TimeSeriesSplit':\n",
    "        splitter = TimeSeriesSplit(n_splits=n_splits)\n",
    "    elif alg == 'custom_cv':\n",
    "        splitter = CustomCV(n_splits=n_splits)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", \"TimeSeriesSplit\", \"custom_cv\" or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    datetimes_train: np.ndarray,  # Add datetimes_train parameter\n",
    "    datetimes_test: np.ndarray,  # Add datetimes_test parameter\n",
    "\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "\n",
    "\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "#         # Removing low variance features\n",
    "#         X_train = exclude_low_variance(X_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "#         # Removing highly correlated features\n",
    "#         X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "#             # Create oversampled datetimes_train\n",
    "#             datetimes_train_oversampled = np.repeat(datetimes_train, sampler.sample_indices_.shape[0], axis=0)\n",
    "\n",
    "    # You can access the underlying model class like this:\n",
    "    if isinstance(estimator, LSTMWrapper):\n",
    "        with _log(f'[{name}] Training LSTM'):\n",
    "            \n",
    "            lstm_model = estimator  # Already an instance of LSTMModel\n",
    "            lstm_model.fit(X_train, y_train, datetimes_train)  # datetimes_train added\n",
    "            result = FoldResult(\n",
    "                name=name,\n",
    "                estimator=lstm_model,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                categories=categories,\n",
    "                datetimes_train=datetimes_train,\n",
    "\n",
    "                datetimes_test=datetimes_test\n",
    "            )\n",
    "            dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "\n",
    "    else:\n",
    "        with _log(f'[{name}] Training'):\n",
    "            estimator = estimator.fit(X_train, y_train)\n",
    "            result = FoldResult(\n",
    "                name=name,\n",
    "                estimator=estimator,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                categories=categories\n",
    "            )\n",
    "            dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    datetimes:  np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    lookback: int = 1  # Add the `lookback` parameter here\n",
    "\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "    \n",
    "    # You can access the underlying model class like this:\n",
    "    if isinstance(estimator, LSTMWrapper):\n",
    "        estimator.set_params(lookback=lookback)\n",
    "    \n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "        datetimes_train, datetimes_test = datetimes[I_train], datetimes[I_test]  # Add datetimes_train and datetimes_test\n",
    "\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            \n",
    "            datetimes_train=datetimes_train,  # Pass datetimes_train\n",
    "            datetimes_test=datetimes_test,  # Pass datetimes_test\n",
    "\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "class LSTMWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lookback=1, batch_size=32):\n",
    "        self.lookback = lookback\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def _preprocess(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray, datetimes: np.ndarray):\n",
    "        datetimes = datetimes.reshape(-1, 1)\n",
    "        time_delta = np.diff(datetimes, axis=0)\n",
    "        time_delta = np.concatenate([np.array([[0]]), time_delta])\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        X_with_time_delta = np.concatenate((X, time_delta), axis=1)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_scaled = scaler.fit_transform(X_with_time_delta)\n",
    "\n",
    "        data_gen = TimeseriesGenerator(\n",
    "            X_scaled, y if y is not None else np.zeros(len(X)),\n",
    "            length=self.lookback,\n",
    "            sampling_rate=1,\n",
    "            stride=1,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        if y is not None:\n",
    "            y_with_time_delta = np.concatenate((y.reshape(-1, 1), np.zeros((len(y), self.lookback - 1))), axis=1)\n",
    "            y_scaled = scaler.fit_transform(y_with_time_delta)\n",
    "\n",
    "            data_gen_y = TimeseriesGenerator(\n",
    "                y_scaled,\n",
    "                y_scaled,\n",
    "                length=self.lookback,\n",
    "                sampling_rate=1,\n",
    "                stride=1,\n",
    "                batch_size=self.batch_size,\n",
    "            )\n",
    "            return iter(data_gen), iter(data_gen_y)\n",
    "\n",
    "        return iter(data_gen)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, datetimes):\n",
    "        X_train_gen, y_train_gen = self._preprocess(X, y, datetimes)\n",
    "        \n",
    "        X_train_batch, y_train_batch = next(X_train_gen), next(y_train_gen)\n",
    "        X_train_shape = X_train_batch[0].shape\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(64, input_shape=(self.lookback, X_train_shape[-1])))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))  # sigmoid activation added\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam')  # change loss to binary_crossentropy\n",
    "\n",
    "        # Loop through all batches and fit the model on each batch\n",
    "        for X_batch, y_batch in zip(X_train_gen, y_train_gen):\n",
    "            self.model.fit(X_batch[0], y_batch[0], epochs=5, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "    def predict(self, X, datetimes):\n",
    "        X_test_gen = self._preprocess(X, None, datetimes)\n",
    "        if X_test_gen is None:\n",
    "            return np.zeros(len(X))  # Return an array of zeros with the same length as X when X_test_gen is None\n",
    "\n",
    "        # Loop through all batches and make predictions on each batch\n",
    "        y_pred = []\n",
    "        for X_batch in X_test_gen:\n",
    "            # Reshape input data to have shape (batch_size, lookback, n_features)\n",
    "#             X_test_shape = X_batch[0].shape\n",
    "#             X_reshaped = X_batch[0].reshape((X_test_shape[0], self.lookback, X_test_shape[-1]))\n",
    "            y_pred.append(self.model.predict(X_batch[0]))\n",
    "        return np.concatenate(y_pred)\n",
    "\n",
    "\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"lookback\": self.lookback}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'proc', 'LABELS_PROC.csv'), index_col=['pcode','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use user speicifc mean threshold\n",
    "\n",
    "# LABELS_PROC['stress_user_mean'] = np.nan\n",
    "\n",
    "# for user in LABELS_PROC.index.get_level_values('pcode').unique():\n",
    "#     user_df = LABELS_PROC.loc[user].copy()\n",
    "#     user_mean = user_df['stress'].mean()\n",
    "#     user_df['stress_user_mean'] = user_df['stress'].apply(lambda x: 1 if x > user_mean else 0)\n",
    "#     LABELS_PROC.loc[user, 'stress_user_mean'] = user_df['stress_user_mean'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 10:56:27,527\tINFO worker.py:1432 -- Connecting to existing Ray cluster at address: 192.168.1.28:6379...\n",
      "2023-06-12 10:56:27,541\tINFO worker.py:1616 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=1608776)\u001b[0m 2023-06-12 10:56:28.621934: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=1608776)\u001b[0m 2023-06-12 10:56:28.623830: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "\u001b[2m\u001b[36m(pid=1608776)\u001b[0m 2023-06-12 10:56:28.659027: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "\u001b[2m\u001b[36m(pid=1608776)\u001b[0m 2023-06-12 10:56:28.659510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=1608776)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=1608776)\u001b[0m 2023-06-12 10:56:29.245801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:29] In progress: [dummy#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:29] Success: [dummy#1] Normalizing numeric features (0.00s).\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:29] In progress: [dummy#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:29] Success: [dummy#1] Training (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:30] In progress: [rf_os#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:30] Success: [rf_os#5] 1-th Feature selection (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m - # Orig. Feat.: 6 (# Cat. = 0; # Num. = 6)\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m - # Sel. Feat.: 4 (# Cat. = 0; # Num. = 4)\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:30] In progress: [rf_os#5] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:30] Success: [rf_os#5] Oversampling (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:34] In progress: [mlp_os#2] Normalizing numeric features.\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1608776)\u001b[0m [23-06-12 10:56:34] Success: [mlp_os#2] Normalizing numeric features (0.00s).\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:33] In progress: [svm_os#3] Training.\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:34] Success: [svm_os#3] Training (0.85s).\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:34] In progress: [mlp_os#3] 1-th Feature selection.\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:34] Success: [mlp_os#3] 1-th Feature selection (0.01s).\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m - # Orig. Feat.: 6 (# Cat. = 0; # Num. = 6)\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m - # Sel. Feat.: 5 (# Cat. = 0; # Num. = 5)\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:34] In progress: [mlp_os#3] Oversampling.\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609156)\u001b[0m [23-06-12 10:56:34] Success: [mlp_os#4] Oversampling (0.01s).\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:37] In progress: [adab_os#4] Normalizing numeric features.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:37] Success: [adab_os#4] Normalizing numeric features (0.01s).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:37] In progress: [adab_os#4] Training.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609152)\u001b[0m [23-06-12 10:56:37] Success: [adab_os#5] Training (0.33s).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:37] In progress: [adab_os#4] 1-th Feature selection.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:37] Success: [adab_os#4] 1-th Feature selection (0.01s).\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m - # Orig. Feat.: 6 (# Cat. = 0; # Num. = 6)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m - # Sel. Feat.: 4 (# Cat. = 0; # Num. = 4)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:37] In progress: [adab_os#4] Oversampling.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1609155)\u001b[0m [23-06-12 10:56:37] Success: [adab_os#4] Oversampling (0.01s).\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1609155)\u001b[0m 2023-06-12 10:56:29.146161: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1609155)\u001b[0m 2023-06-12 10:56:29.183225: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1609155)\u001b[0m 2023-06-12 10:56:29.183701: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1609155)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1609155)\u001b[0m 2023-06-12 10:56:29.769348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "lookback = 1\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "ESTIMATOR_LR = LogisticRegression(random_state = RANDOM_STATE, max_iter=500 )\n",
    "ESTIMATOR_KNN = KNeighborsClassifier()\n",
    "ESTIMATOR_SVM = SVC(probability=True)\n",
    "ESTIMATOR_GP = GaussianProcessClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_DT = DecisionTreeClassifier(random_state = RANDOM_STATE)\n",
    "ESTIMATOR_MLP = MLPClassifier(random_state=RANDOM_STATE, max_iter=2000)\n",
    "ESTIMATOR_ADAB = AdaBoostClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_GNB = GaussianNB()\n",
    "ESTIMATOR_QDA = QuadraticDiscriminantAnalysis()\n",
    "ESTIMATOR_LSTM = LSTMWrapper()\n",
    "\n",
    "  \n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \n",
    "#         estimator=LogisticRegression(\n",
    "#         penalty='l1' \n",
    "#         ,solver='liblinear'\n",
    "#         , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "#     ),\n",
    "    threshold=1e-5\n",
    "#     threshold = 0.005\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_DUMMY),\n",
    "        oversample=False,\n",
    "        select=None,\n",
    "        name='dummy'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_LR),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='lr_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_KNN),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='knn_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_SVM),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='svm_os'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_DT),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='dt_os'\n",
    "#     ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_MLP),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='mlp_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_ADAB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='adab_os'\n",
    "    ),\n",
    "#         dict(\n",
    "#         estimator=clone(ESTIMATOR_GP),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gp_os'\n",
    "#     ),\n",
    "#             dict(\n",
    "#         estimator=clone(ESTIMATOR_GNB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gnb_os'\n",
    "#     ),\n",
    "#             dict(\n",
    "#         estimator=clone(ESTIMATOR_QDA),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='qda_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_LSTM),\n",
    "#         oversample=None,\n",
    "#         lookback=lookback,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='lstm_os'\n",
    "#     )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress.pkl')\n",
    "par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', 'stress')\n",
    "\n",
    "if os.path.isdir(par_dir):\n",
    "    # Get a list of all the files in the folder\n",
    "    files = os.listdir(par_dir)\n",
    "\n",
    "    # Delete all the files in the folder\n",
    "    for file in files:\n",
    "        if file !='.ipynb_checkpoints':\n",
    "            os.remove(os.path.join(par_dir, file))\n",
    "os.makedirs(par_dir, exist_ok=True)\n",
    "\n",
    "#with on_ray(num_cpus=6):\n",
    "with on_ray():\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):       \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        ################################################\n",
    "        #Use user speicifc mean threshold\n",
    "#         y =LABELS_PROC['stress_user_mean'].to_numpy()\n",
    "        #Use fixed threshold\n",
    "#         y =LABELS_PROC['stress_fixed'].to_numpy()\n",
    "        #Use three categories (fixed threshold)\n",
    "#        y =LABELS_PROC['stress_fixed_tri'].to_numpy()\n",
    "\n",
    "        \n",
    "        #The following code is designed for reordering for the sake of time series split\n",
    "        #################################################\n",
    "        # Create a DataFrame with user_id and datetime\n",
    "        df = pd.DataFrame({'user_id': groups, 'datetime': pd.to_datetime(datetimes)})\n",
    "\n",
    "        # Normalize the datetime for each user\n",
    "#         df['datetime'] = df.groupby('user_id')['datetime'].transform(lambda x: x - x.min())\n",
    "        df['datetime'] = df.groupby('user_id')['datetime'].transform(lambda x: x - x.min().normalize())\n",
    "\n",
    "        # Sort the DataFrame by datetime\n",
    "#         df = df.sort_values(by=['user_id', 'datetime'])\n",
    "        df = df.sort_values(by=[ 'datetime'])\n",
    "        # Shuffle the DataFrame\n",
    "#         df = df.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "        # Update groups and datetimes\n",
    "        groups = df['user_id'].to_numpy()\n",
    "        datetimes = df['datetime'].dt.total_seconds().to_numpy()  # convert to seconds\n",
    "\n",
    "        # Use the new order to reorder X and y\n",
    "        X = X.reindex(df.index)\n",
    "        y = y[df.index]\n",
    "#         ###################################################\n",
    "        \n",
    "\n",
    "        #The following code is for only using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[~X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[~y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[~groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        #The following code is for excluding using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        \n",
    "        \n",
    "        ###########################################\n",
    "        #The following code is for similar-user model\n",
    "        ###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "        ###########################################\n",
    "        #Remove low frequency features\n",
    "#         mask = ['CAE#', 'MED#', 'ONF#', 'PWS#', 'RNG#','MSG#' ]\n",
    "#         X = X.loc[:, [all(m not in str(x) for m in mask) for x in X.columns]]\n",
    "\n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([ feat_pif, feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "        #The following code is for calculating aggregated features\n",
    "        ########################################################################\n",
    "        # Define a function to split the column name into sensor and attribute\n",
    "#         def split_column_name(col_name):\n",
    "#             parts = col_name.rsplit(\"#\", 1)  # Split on last occurrence of '#'\n",
    "#             return parts[0]  # This gives you 'Sensor#Attribute'\n",
    "\n",
    "#         # Get a list of unique sensor-attribute combinations\n",
    "#         df=feat_today_sensor\n",
    "#         sensor_attributes = df.columns.map(split_column_name).unique()\n",
    "\n",
    "#         # Create a list to hold the aggregated results\n",
    "#         agg_results = []\n",
    "\n",
    "#         # Loop over each sensor-attribute, select the appropriate columns, compute the mean and std\n",
    "#         for sensor_attribute in sensor_attributes:\n",
    "#             # Select columns for this sensor-attribute\n",
    "#             cols_to_aggregate = [col for col in df.columns if col.startswith(sensor_attribute)]\n",
    "#             # Compute the mean and std and store in the new DataFrame\n",
    "#             agg_results.append(df[cols_to_aggregate].mean(axis=1).rename(sensor_attribute + '|'+ 'MEAN'))\n",
    "#             agg_results.append(df[cols_to_aggregate].std(axis=1).rename(sensor_attribute + '|'+'STD'))\n",
    "\n",
    "#         # Concatenate all the results into a single DataFrame\n",
    "#         agg_feature = pd.concat(agg_results, axis=1)\n",
    "        #########################################################################\n",
    "        feat_final = pd.concat([    feat_yesterday_ESM],axis=1)\n",
    "        X = feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        \n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups, datetimes =datetimes,\n",
    "            path=par_dir,\n",
    "            normalize=True,\n",
    "            split='groupk',\n",
    "            categories=cats,\n",
    "            split_params={'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation coefficient\n",
    "r = np.corrcoef(feat_current_ESM['ESM#LastLabel_3'], y)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3218427521099164"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        if y_prob is not None:\n",
    "            for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "                R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                    y_true=y_true, y_score=y_prob,\n",
    "                    average=avg, multi_class=mc, labels=classes\n",
    "                ) if not is_same_y else np.nan\n",
    "        else:\n",
    "            for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "                R[f'roauc_{avg}_{mc}'] = np.nan\n",
    "    else:\n",
    "        if y_prob is not None:\n",
    "            R[f'roauc'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_prob[:, 1], average=None\n",
    "            ) if not is_same_y else np.nan\n",
    "            for i, c in enumerate(classes):\n",
    "                R[f'prauc_{c}'] = average_precision_score(\n",
    "                    y_true=y_true, y_score=y_prob[:, i], pos_label=c, average=None\n",
    "                ) \n",
    "                R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "        else:\n",
    "            R[f'roauc'] = np.nan\n",
    "            for c in classes:\n",
    "                R[f'prauc_{c}'] = np.nan\n",
    "                R[f'prauc_ref_{c}'] = np.nan\n",
    "\n",
    "    # Probability Measure\n",
    "    if y_prob is not None:\n",
    "        R['log_loss'] = log_loss(y_true=y_true, y_pred=y_prob, labels=classes, normalize=True)\n",
    "        if not is_multiclass:\n",
    "            R[f'brier_loss'] = brier_score_loss(\n",
    "                y_true=y_true, y_prob=y_prob[:, 1], pos_label=classes[1]\n",
    "            )\n",
    "    else:\n",
    "        R['log_loss'] = np.nan\n",
    "        if not is_multiclass:\n",
    "            R[f'brier_loss'] = np.nan\n",
    "\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>split</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_class_ratio</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>...</th>\n",
       "      <th>train_pre_1</th>\n",
       "      <th>train_rec_1</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_roauc</th>\n",
       "      <th>train_prauc_0</th>\n",
       "      <th>train_prauc_ref_0</th>\n",
       "      <th>train_prauc_1</th>\n",
       "      <th>train_prauc_ref_1</th>\n",
       "      <th>train_log_loss</th>\n",
       "      <th>train_brier_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>knn_os</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>254</td>\n",
       "      <td>246</td>\n",
       "      <td>1.032520</td>\n",
       "      <td>93</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633100</td>\n",
       "      <td>0.651939</td>\n",
       "      <td>0.642381</td>\n",
       "      <td>0.695230</td>\n",
       "      <td>0.643060</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.671520</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.734805</td>\n",
       "      <td>0.227574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>526</td>\n",
       "      <td>236</td>\n",
       "      <td>290</td>\n",
       "      <td>0.813793</td>\n",
       "      <td>149</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.538967</td>\n",
       "      <td>0.581266</td>\n",
       "      <td>0.648964</td>\n",
       "      <td>0.604845</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.674416</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>mlp_os</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>533</td>\n",
       "      <td>259</td>\n",
       "      <td>274</td>\n",
       "      <td>0.945255</td>\n",
       "      <td>175</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716107</td>\n",
       "      <td>0.571693</td>\n",
       "      <td>0.635802</td>\n",
       "      <td>0.737981</td>\n",
       "      <td>0.699549</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.739182</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.598510</td>\n",
       "      <td>0.206085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>mlp_os</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>526</td>\n",
       "      <td>236</td>\n",
       "      <td>290</td>\n",
       "      <td>0.813793</td>\n",
       "      <td>166</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725100</td>\n",
       "      <td>0.512676</td>\n",
       "      <td>0.600660</td>\n",
       "      <td>0.729147</td>\n",
       "      <td>0.700584</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.737221</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.601507</td>\n",
       "      <td>0.208090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>lr_os</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>540</td>\n",
       "      <td>268</td>\n",
       "      <td>272</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>158</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614858</td>\n",
       "      <td>0.580794</td>\n",
       "      <td>0.597341</td>\n",
       "      <td>0.649217</td>\n",
       "      <td>0.589244</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.683546</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.651182</td>\n",
       "      <td>0.229959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label      alg split  n_feature  test_inst  test_inst_0  test_inst_1  \\\n",
       "0  stress   knn_os     5          4        500          254          246   \n",
       "1  stress  adab_os     1          5        526          236          290   \n",
       "2  stress   mlp_os     3          5        533          259          274   \n",
       "3  stress   mlp_os     1          5        526          236          290   \n",
       "4  stress    lr_os     2          5        540          268          272   \n",
       "\n",
       "   test_class_ratio  test_true_0_pred_0  test_true_0_pred_1  ...  train_pre_1  \\\n",
       "0          1.032520                  93                 161  ...     0.633100   \n",
       "1          0.813793                 149                  87  ...     0.630769   \n",
       "2          0.945255                 175                  84  ...     0.716107   \n",
       "3          0.813793                 166                  70  ...     0.725100   \n",
       "4          0.985294                 158                 110  ...     0.614858   \n",
       "\n",
       "   train_rec_1  train_f1_1  train_roauc  train_prauc_0  train_prauc_ref_0  \\\n",
       "0     0.651939    0.642381     0.695230       0.643060                0.5   \n",
       "1     0.538967    0.581266     0.648964       0.604845                0.5   \n",
       "2     0.571693    0.635802     0.737981       0.699549                0.5   \n",
       "3     0.512676    0.600660     0.729147       0.700584                0.5   \n",
       "4     0.580794    0.597341     0.649217       0.589244                0.5   \n",
       "\n",
       "   train_prauc_1  train_prauc_ref_1  train_log_loss  train_brier_loss  \n",
       "0       0.671520                0.5        1.734805          0.227574  \n",
       "1       0.674416                0.5        0.691300          0.249077  \n",
       "2       0.739182                0.5        0.598510          0.206085  \n",
       "3       0.737221                0.5        0.601507          0.208090  \n",
       "4       0.683546                0.5        0.651182          0.229959  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "threshold = 0.5\n",
    "\n",
    "# Loop through the desired labels\n",
    "for l in ['stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "\n",
    "    for f in os.listdir(dir_l):\n",
    "        if f == '.ipynb_checkpoints':\n",
    "            continue\n",
    "\n",
    "        model, pid = f[:f.index('.pkl')].split('#')\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "        X, y = res.X_test, res.y_test\n",
    "\n",
    "        # Check if the classifier is LSTM-based\n",
    "        if isinstance(res.estimator, LSTMWrapper) or res.estimator.__class__.__name__ == \"LSTMWrapper\":\n",
    "            # Extract datetimes and perform predictions\n",
    "            datetimes = res.datetimes_test\n",
    "            y_prob = res.estimator.predict(X, datetimes)\n",
    "            y_pred = np.where(y_prob > threshold, 1, 0)\n",
    "            \n",
    "            \n",
    "            y = y[lookback:]\n",
    "\n",
    "            \n",
    "            # Transform y_prob into a two-dimensional array with two columns\n",
    "            row_numbers = np.arange(len(y_prob))\n",
    "            y_prob = np.column_stack((row_numbers, y_prob))\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Perform predictions for other classifiers (without datetimes)\n",
    "            y_pred = res.estimator.predict(X)\n",
    "\n",
    "            if hasattr(res.estimator, 'predict_proba'):\n",
    "                y_prob = res.estimator.predict_proba(X)\n",
    "            else:\n",
    "                y_prob = None\n",
    "\n",
    "        ev_test = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        X, y = res.X_train, res.y_train\n",
    "\n",
    "        # Check if the classifier is LSTM-based\n",
    "        if isinstance(res.estimator, LSTMWrapper) or res.estimator.__class__.__name__ == \"LSTMWrapper\":\n",
    "            # Extract datetimes and perform predictions\n",
    "            datetimes = res.datetimes_train\n",
    "            y_prob = res.estimator.predict(X, datetimes)\n",
    "            y_pred = np.where(y_prob > threshold, 1, 0)\n",
    "            \n",
    "            y = y[lookback:]\n",
    "\n",
    "            \n",
    "            # Transform y_prob into a two-dimensional array with two columns\n",
    "            row_numbers = np.arange(len(y_prob))\n",
    "            y_prob = np.column_stack((row_numbers, y_prob))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Perform predictions for other classifiers (without datetimes)\n",
    "            y_pred = res.estimator.predict(X)\n",
    "\n",
    "            if hasattr(res.estimator, 'predict_proba'):\n",
    "                y_prob = res.estimator.predict_proba(X)\n",
    "            else:\n",
    "                y_prob = None\n",
    "        \n",
    "        \n",
    "        ev_train = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        RESULTS_EVAL.append({\n",
    "            'label': l,\n",
    "            'alg': model,\n",
    "            'split': pid,\n",
    "            'n_feature': len(X.columns),\n",
    "            **{f'test_{k}': v for k, v in ev_test.items()},\n",
    "            **{f'train_{k}': v for k, v in ev_train.items()}\n",
    "        })\n",
    "\n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_xgbos = RESULTS_EVAL[RESULTS_EVAL['alg']=='lr_os']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_bac</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_roauc</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>test_true_1_pred_0</th>\n",
       "      <th>test_true_1_pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>540</td>\n",
       "      <td>268</td>\n",
       "      <td>272</td>\n",
       "      <td>0.609259</td>\n",
       "      <td>0.609114</td>\n",
       "      <td>0.609033</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>158</td>\n",
       "      <td>110</td>\n",
       "      <td>101</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>533</td>\n",
       "      <td>259</td>\n",
       "      <td>274</td>\n",
       "      <td>0.559099</td>\n",
       "      <td>0.560705</td>\n",
       "      <td>0.558347</td>\n",
       "      <td>0.586753</td>\n",
       "      <td>160</td>\n",
       "      <td>99</td>\n",
       "      <td>136</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>526</td>\n",
       "      <td>236</td>\n",
       "      <td>290</td>\n",
       "      <td>0.625475</td>\n",
       "      <td>0.625628</td>\n",
       "      <td>0.623995</td>\n",
       "      <td>0.653865</td>\n",
       "      <td>148</td>\n",
       "      <td>88</td>\n",
       "      <td>109</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>254</td>\n",
       "      <td>246</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.643957</td>\n",
       "      <td>0.639128</td>\n",
       "      <td>0.668267</td>\n",
       "      <td>196</td>\n",
       "      <td>58</td>\n",
       "      <td>119</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>520</td>\n",
       "      <td>247</td>\n",
       "      <td>273</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.596877</td>\n",
       "      <td>0.596058</td>\n",
       "      <td>0.633077</td>\n",
       "      <td>151</td>\n",
       "      <td>96</td>\n",
       "      <td>114</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  test_inst  test_inst_0  test_inst_1  test_acc  test_bac  \\\n",
       "4      2        540          268          272  0.609259  0.609114   \n",
       "6      3        533          259          274  0.559099  0.560705   \n",
       "27     1        526          236          290  0.625475  0.625628   \n",
       "34     5        500          254          246  0.646000  0.643957   \n",
       "38     4        520          247          273  0.596154  0.596877   \n",
       "\n",
       "    test_f1_macro  test_roauc  test_true_0_pred_0  test_true_0_pred_1  \\\n",
       "4        0.609033    0.602200                 158                 110   \n",
       "6        0.558347    0.586753                 160                  99   \n",
       "27       0.623995    0.653865                 148                  88   \n",
       "34       0.639128    0.668267                 196                  58   \n",
       "38       0.596058    0.633077                 151                  96   \n",
       "\n",
       "    test_true_1_pred_0  test_true_1_pred_1  \n",
       "4                  101                 171  \n",
       "6                  136                 138  \n",
       "27                 109                 181  \n",
       "34                 119                 127  \n",
       "38                 114                 159  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_xgbos[['split','test_inst','test_inst_0','test_inst_1','test_acc','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "       'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_performance = RESULTS_xgbos['test_f1_macro'] >= 0.761\n",
    "# RESULTS_xgbos_high_performance = RESULTS_xgbos[mask_performance]\n",
    "# RESULTS_xgbos_low_performance = RESULTS_xgbos[~mask_performance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS_xgbos_low_performance[['split','test_inst','test_inst_0','test_inst_1','test_acc','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "#        'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS_xgbos_high_performance[['split','test_inst','test_inst_0','test_inst_1','test_acc','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "#        'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code is for cluster analysis\n",
    "# pcode_to_cluster = dict(zip(similar_user['pcode'], similar_user['cluster']))\n",
    "# RESULTS_EVAL['cluster'] = RESULTS_EVAL['split'].map(pcode_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>metric</th>\n",
       "      <th>n</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>value_count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>med</th>\n",
       "      <th>range</th>\n",
       "      <th>conf.</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>split</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1:1, 4:1, 5:1, 2:1, 3:1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>n_feature</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>(3.9199126193417424, 5.280087380658257)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>test_inst</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>523.8</td>\n",
       "      <td>15.270887</td>\n",
       "      <td>526.0</td>\n",
       "      <td>(500, 540)</td>\n",
       "      <td>(504.838688975434, 542.7613110245659)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>test_inst_0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>252.8</td>\n",
       "      <td>12.111978</td>\n",
       "      <td>254.0</td>\n",
       "      <td>(236, 268)</td>\n",
       "      <td>(237.7609935379139, 267.8390064620861)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>test_inst_1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>15.811388</td>\n",
       "      <td>273.0</td>\n",
       "      <td>(246, 290)</td>\n",
       "      <td>(251.3675683852244, 290.6324316147756)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label      alg       metric  n  cardinality              value_count  \\\n",
       "0  stress  adab_os        split  5          5.0  1:1, 4:1, 5:1, 2:1, 3:1   \n",
       "1  stress  adab_os    n_feature  5          NaN                      NaN   \n",
       "2  stress  adab_os    test_inst  5          NaN                      NaN   \n",
       "3  stress  adab_os  test_inst_0  5          NaN                      NaN   \n",
       "4  stress  adab_os  test_inst_1  5          NaN                      NaN   \n",
       "\n",
       "      sum   mean         SD    med       range  \\\n",
       "0     NaN    NaN        NaN    NaN         NaN   \n",
       "1    23.0    4.6   0.547723    5.0      (4, 5)   \n",
       "2  2619.0  523.8  15.270887  526.0  (500, 540)   \n",
       "3  1264.0  252.8  12.111978  254.0  (236, 268)   \n",
       "4  1355.0  271.0  15.811388  273.0  (246, 290)   \n",
       "\n",
       "                                     conf.  nan_count  \n",
       "0                                      NaN        NaN  \n",
       "1  (3.9199126193417424, 5.280087380658257)        0.0  \n",
       "2    (504.838688975434, 542.7613110245659)        0.0  \n",
       "3   (237.7609935379139, 267.8390064620861)        0.0  \n",
       "4   (251.3675683852244, 290.6324316147756)        0.0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "#    ['label', 'alg', 'cluster']\n",
    "     ['label', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                alg=row.alg,\n",
    "#                 cluster = row.cluster,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">mean_sd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_bac</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_roauc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">stress</th>\n",
       "      <th>dummy</th>\n",
       "      <td>6.0 (0.0)</td>\n",
       "      <td>0.517 (0.023)</td>\n",
       "      <td>0.5 (0.0)</td>\n",
       "      <td>0.341 (0.01)</td>\n",
       "      <td>0.5 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adab_os</th>\n",
       "      <td>4.6 (0.548)</td>\n",
       "      <td>0.587 (0.031)</td>\n",
       "      <td>0.587 (0.03)</td>\n",
       "      <td>0.584 (0.032)</td>\n",
       "      <td>0.624 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn_os</th>\n",
       "      <td>4.6 (0.548)</td>\n",
       "      <td>0.533 (0.031)</td>\n",
       "      <td>0.533 (0.032)</td>\n",
       "      <td>0.528 (0.033)</td>\n",
       "      <td>0.548 (0.051)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_os</th>\n",
       "      <td>4.6 (0.548)</td>\n",
       "      <td>0.607 (0.033)</td>\n",
       "      <td>0.607 (0.031)</td>\n",
       "      <td>0.605 (0.031)</td>\n",
       "      <td>0.629 (0.034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_os</th>\n",
       "      <td>4.6 (0.548)</td>\n",
       "      <td>0.592 (0.024)</td>\n",
       "      <td>0.594 (0.025)</td>\n",
       "      <td>0.582 (0.025)</td>\n",
       "      <td>0.615 (0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_os</th>\n",
       "      <td>4.6 (0.548)</td>\n",
       "      <td>0.578 (0.02)</td>\n",
       "      <td>0.581 (0.024)</td>\n",
       "      <td>0.574 (0.024)</td>\n",
       "      <td>0.602 (0.037)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_os</th>\n",
       "      <td>4.6 (0.548)</td>\n",
       "      <td>0.596 (0.014)</td>\n",
       "      <td>0.601 (0.018)</td>\n",
       "      <td>0.581 (0.024)</td>\n",
       "      <td>0.62 (0.014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_os</th>\n",
       "      <td>4.6 (0.548)</td>\n",
       "      <td>0.575 (0.008)</td>\n",
       "      <td>0.58 (0.014)</td>\n",
       "      <td>0.563 (0.017)</td>\n",
       "      <td>0.596 (0.034)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean_sd                                               \\\n",
       "metric            n_feature       test_acc       test_bac  test_f1_macro   \n",
       "label  alg                                                                 \n",
       "stress dummy      6.0 (0.0)  0.517 (0.023)      0.5 (0.0)   0.341 (0.01)   \n",
       "       adab_os  4.6 (0.548)  0.587 (0.031)   0.587 (0.03)  0.584 (0.032)   \n",
       "       knn_os   4.6 (0.548)  0.533 (0.031)  0.533 (0.032)  0.528 (0.033)   \n",
       "       lr_os    4.6 (0.548)  0.607 (0.033)  0.607 (0.031)  0.605 (0.031)   \n",
       "       mlp_os   4.6 (0.548)  0.592 (0.024)  0.594 (0.025)  0.582 (0.025)   \n",
       "       rf_os    4.6 (0.548)   0.578 (0.02)  0.581 (0.024)  0.574 (0.024)   \n",
       "       svm_os   4.6 (0.548)  0.596 (0.014)  0.601 (0.018)  0.581 (0.024)   \n",
       "       xgb_os   4.6 (0.548)  0.575 (0.008)   0.58 (0.014)  0.563 (0.017)   \n",
       "\n",
       "                               \n",
       "metric             test_roauc  \n",
       "label  alg                     \n",
       "stress dummy        0.5 (0.0)  \n",
       "       adab_os  0.624 (0.031)  \n",
       "       knn_os   0.548 (0.051)  \n",
       "       lr_os    0.629 (0.034)  \n",
       "       mlp_os   0.615 (0.026)  \n",
       "       rf_os    0.602 (0.037)  \n",
       "       svm_os    0.62 (0.014)  \n",
       "       xgb_os   0.596 (0.034)  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "        ['n_feature','test_acc' ,'test_bac',  'test_f1_macro','test_roauc']\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep=''),\n",
    ").pivot(\n",
    "    index=['label', 'alg'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "\n",
    "# separate rows where 'alg' is 'dummy' and 'alg' is not 'dummy'\n",
    "df_dummy = SUB_SUMMARY_EVAL[SUB_SUMMARY_EVAL.index.get_level_values('alg') == 'dummy']\n",
    "df_others = SUB_SUMMARY_EVAL[SUB_SUMMARY_EVAL.index.get_level_values('alg') != 'dummy']\n",
    "\n",
    "# concatenate them ensuring that 'dummy' rows are always at the top for each group\n",
    "SUB_SUMMARY_EVAL = pd.concat([df_dummy, df_others])\n",
    "\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "# for l in ['valence', 'arousal', 'disturbance', 'stress']:\n",
    "for l in ['stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        if f!='.ipynb_checkpoints':\n",
    "            res = load(os.path.join(dir_l, f))\n",
    "\n",
    "            f_norm = f[:f.index('.pkl')]\n",
    "            alg = f_norm[:f.rindex('#')]\n",
    "\n",
    "            feat_imp = feature_importance(res.estimator)\n",
    "            if not feat_imp:\n",
    "                continue\n",
    "\n",
    "            names, importance = feat_imp\n",
    "            new_names = []\n",
    "            for n in names:\n",
    "                for c in res.categories:\n",
    "                    n = n.replace(f'{c}_', f'{c}=')\n",
    "                new_names.append(n)\n",
    "\n",
    "            d = pd.DataFrame(\n",
    "                importance.reshape(1, -1),\n",
    "                columns=new_names\n",
    "            )\n",
    "            IMPORTANCE_EVAL[(l, alg)].append(d)\n",
    "        \n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for (l, alg), v in IMPORTANCE_EVAL.items():\n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        alg=alg\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attaching packages  tidyverse 1.3.2 \n",
      " ggplot2 3.4.0       purrr   1.0.0 \n",
      " tibble  3.1.8       dplyr   1.0.10\n",
      " tidyr   1.2.1       stringr 1.5.0 \n",
      " readr   2.1.3       forcats 0.5.2 \n",
      " Conflicts  tidyverse_conflicts() \n",
      " dplyr::filter() masks stats::filter()\n",
      " dplyr::lag()    masks stats::lag()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: sysfonts\n",
      "\n",
      "R[write to console]: Loading required package: showtextdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(tidyverse)\n",
    "library(ggforce)\n",
    "library(ggpubr)\n",
    "library(showtext)\n",
    "library(rmcorr)\n",
    "library(patchwork)\n",
    "\n",
    "THEME_DEFAULT <- theme_bw(\n",
    "    base_size=10\n",
    ") + theme(\n",
    "        axis.title.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.title.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.text.x=element_text(colour='grey20', size=10),\n",
    "        axis.text.y=element_text(colour='grey20', size=10),\n",
    "        strip.text.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        strip.text.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.background=element_blank(),\n",
    "        legend.title=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.text=element_text(colour='grey20', size=10),\n",
    "        legend.position='top',\n",
    "        legend.box.spacing= unit(0, 'cm'),\n",
    "        plot.subtitle=element_text(colour='grey20', size=10, hjust=.5)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iclab1209/miniconda3/envs/sci-data/lib/python3.9/site-packages/rpy2/robjects/pandas2ri.py:65: UserWarning: Error while trying to convert the column \"feature\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'numpy.str_'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 32 -u cm\n",
    "\n",
    "plots <- list()\n",
    "\n",
    "#for (l in c('valence', 'arousal', 'stress', 'disturbance')) {\n",
    "for (l in c( 'stress')) {\n",
    "    data <- IMPORTANCE_SUMMARY %>% filter(\n",
    "        (label == l)\n",
    "    )\n",
    "\n",
    "    p_label <- ggplot() + geom_text(\n",
    "        aes(x=.5, y=.5),\n",
    "        label=str_to_title(l), \n",
    "        family='ssp', \n",
    "        fontface='bold',\n",
    "        size=4\n",
    "    ) + theme_void()\n",
    "\n",
    "    p_rf <- ggplot(\n",
    "        data %>% filter(alg == 'rf_os') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1, vjust=.5),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='Random Forest'\n",
    "    )\n",
    "    \n",
    "    p_xgb <- ggplot(\n",
    "        data %>% filter(alg == 'xgb_os') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1, vjust=.5),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='XGBoost'\n",
    "    )\n",
    "    \n",
    "    plots[[paste(l, 'label', sep='_')]] <- p_label\n",
    "    plots[[paste(l, 'rf', sep='_')]] <- p_rf\n",
    "    plots[[paste(l, 'xgb', sep='_')]] <- p_xgb\n",
    "}\n",
    "\n",
    "#p <- plots$arousal_label + plots$valence_label\n",
    "#p <- p / (plots$arousal_rf | plots$arousal_xgb | plots$valence_rf | plots$valence_xgb)\n",
    "#p <- p / (plots$stress_label + plots$disturbance_label)\n",
    "#p <- p / (plots$stress_rf | plots$stress_xgb | plots$disturbance_rf | plots$disturbance_xgb)\n",
    "p <- plots$stress_label \n",
    "p <- p / (plots$stress_rf | plots$stress_xgb)\n",
    "\n",
    "p <- p + plot_layout(\n",
    "    heights=c(1.1, 10, 1.1, 10)\n",
    ")\n",
    "\n",
    "ggsave(paste('./fig/imp.pdf'), plot=p, width=26, height=32, unit='cm', device=cairo_pdf)\n",
    "print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
