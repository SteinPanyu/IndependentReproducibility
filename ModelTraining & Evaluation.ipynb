{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = GroupKFold(n_splits=n_splits \n",
    ")\n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 15:19:24,355\tINFO worker.py:1432 -- Connecting to existing Ray cluster at address: 192.168.1.28:6379...\n",
      "2023-06-06 15:19:24,412\tINFO worker.py:1616 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m [23-06-06 15:19:26] In progress: [dummy#P15] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=112784)\u001b[0m [23-06-06 15:19:26] Success: [dummy#P02] Normalizing numeric features (0.17s).\n",
      "\u001b[2m\u001b[36m(_train pid=112784)\u001b[0m [23-06-06 15:19:26] In progress: [dummy#P02] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:27] Success: [dummy#P45] Training (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:27] Success: [dummy#P48] Training (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:28] Success: [dummy#P49] Training (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:31] In progress: [dummy#P60] Normalizing numeric features.\u001b[32m [repeated 36x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:31] Success: [dummy#P60] Normalizing numeric features (0.09s).\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:31] In progress: [dummy#P60] Training.\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:31] Success: [dummy#P61] Training (0.03s).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153497, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:34] In progress: [dummy#P80] Normalizing numeric features.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153497, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:34] Success: [dummy#P80] Normalizing numeric features (0.08s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153497, ip=192.168.1.27)\u001b[0m [23-06-06 15:19:34] In progress: [dummy#P80] Training.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112784)\u001b[0m [23-06-06 15:19:37] Success: [dummy#P02] Training (11.03s).\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112791)\u001b[0m [23-06-06 15:19:42] Success: [dummy#P03] Training (15.96s).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112812)\u001b[0m [23-06-06 15:19:47] Success: [dummy#P08] Training (19.99s).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112969)\u001b[0m [23-06-06 15:19:52] Success: [dummy#P13] Training (25.04s).\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113619)\u001b[0m [23-06-06 15:19:58] In progress: [rf_ns#P01] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m [23-06-06 15:19:57] Success: [dummy#P35] Training (29.48s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m [23-06-06 15:19:58] Success: [rf_ns#P02] Normalizing numeric features (0.12s).\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m [23-06-06 15:19:58] In progress: [rf_ns#P02] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m [23-06-06 15:19:59] Success: [rf_ns#P02] 1-th Feature selection (1.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m - # Sel. Feat.: 166 (# Cat. = 0; # Num. = 166)\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m [23-06-06 15:19:59] In progress: [rf_ns#P02] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:02] In progress: [rf_ns#P53] Normalizing numeric features.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113213)\u001b[0m [23-06-06 15:20:03] Success: [rf_ns#P09] Training (3.56s).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:02] Success: [rf_ns#P53] Normalizing numeric features (0.14s).\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:02] In progress: [rf_ns#P53] 1-th Feature selection.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:03] Success: [rf_ns#P53] 1-th Feature selection (0.41s).\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 148 (# Cat. = 0; # Num. = 148)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:03] In progress: [rf_ns#P53] Training.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:06] In progress: [rf_ns#P76] Normalizing numeric features.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112812)\u001b[0m [23-06-06 15:20:08] Success: [rf_ns#P28] Training (8.60s).\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:07] Success: [rf_ns#P76] Normalizing numeric features (0.12s).\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:07] In progress: [rf_ns#P76] 1-th Feature selection.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:07] Success: [rf_ns#P76] 1-th Feature selection (0.48s).\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 151 (# Cat. = 0; # Num. = 151)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:07] In progress: [rf_ns#P76] Training.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112791)\u001b[0m [23-06-06 15:20:13] Success: [rf_ns#P35] Training (13.91s).\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m [23-06-06 15:20:19] Success: [rf_ns#P40] Training (19.38s).\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113213)\u001b[0m [23-06-06 15:20:21] In progress: [rf_os#P01] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=113213)\u001b[0m [23-06-06 15:20:21] Success: [rf_os#P01] Normalizing numeric features (0.15s).\n",
      "\u001b[2m\u001b[36m(_train pid=113213)\u001b[0m [23-06-06 15:20:21] In progress: [rf_os#P01] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m [23-06-06 15:20:22] Success: [rf_os#P05] 1-th Feature selection (1.17s).\n",
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\n",
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m - # Sel. Feat.: 182 (# Cat. = 0; # Num. = 182)\n",
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m [23-06-06 15:20:22] In progress: [rf_os#P05] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m [23-06-06 15:20:22] Success: [rf_os#P05] Oversampling (0.17s).\n",
      "\u001b[2m\u001b[36m(_train pid=112776)\u001b[0m [23-06-06 15:20:22] In progress: [rf_os#P05] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:24] Success: [rf_os#P47] Training (1.30s).\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154639, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:26] In progress: [rf_os#P55] Normalizing numeric features.\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:26] Success: [rf_os#P57] Normalizing numeric features (0.10s).\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:26] In progress: [rf_os#P57] 1-th Feature selection.\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154512, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:27] Success: [rf_os#P60] 1-th Feature selection (0.68s).\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154512, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154512, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 173 (# Cat. = 0; # Num. = 173)\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154512, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:27] In progress: [rf_os#P60] Oversampling.\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154512, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:27] Success: [rf_os#P60] Oversampling (0.10s).\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154512, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:27] In progress: [rf_os#P60] Training.\u001b[32m [repeated 34x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=112805)\u001b[0m [23-06-06 15:20:29] Success: [rf_os#P23] Training (6.41s).\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27236, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:30] In progress: [rf_os#P77] Normalizing numeric features.\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27236, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:30] Success: [rf_os#P77] Normalizing numeric features (0.11s).\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27236, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:30] In progress: [rf_os#P77] 1-th Feature selection.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27374, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:31] Success: [rf_os#P76] 1-th Feature selection (0.45s).\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27374, ip=192.168.1.28)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27374, ip=192.168.1.28)\u001b[0m - # Sel. Feat.: 151 (# Cat. = 0; # Num. = 151)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27374, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:31] In progress: [rf_os#P76] Oversampling.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27374, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:32] Success: [rf_os#P76] Oversampling (0.06s).\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27374, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:32] In progress: [rf_os#P76] Training.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=27374, ip=192.168.1.28)\u001b[0m [23-06-06 15:20:34] Success: [rf_os#P76] Training (1.97s).\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:36] In progress: [xgb_ns#P47] Normalizing numeric features.\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:36] Success: [xgb_ns#P47] Normalizing numeric features (0.09s).\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:36] In progress: [xgb_ns#P47] 1-th Feature selection.\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:38] Success: [xgb_ns#P51] 1-th Feature selection (0.37s).\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 158 (# Cat. = 0; # Num. = 158)\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154226, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:38] In progress: [xgb_ns#P51] Training.\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:39] Success: [xgb_ns#P52] Training (1.11s).\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113382)\u001b[0m [23-06-06 15:20:40] In progress: [xgb_ns#P80] Normalizing numeric features.\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:41] Success: [xgb_ns#P78] Normalizing numeric features (0.22s).\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:41] In progress: [xgb_ns#P78] 1-th Feature selection.\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:41] Success: [xgb_ns#P78] 1-th Feature selection (0.81s).\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 174 (# Cat. = 0; # Num. = 174)\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:41] In progress: [xgb_ns#P78] Training.\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113619)\u001b[0m [23-06-06 15:20:45] Success: [xgb_ns#P30] Training (8.79s).\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113440)\u001b[0m [23-06-06 15:20:46] In progress: [xgb_os#P42] Normalizing numeric features.\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112791)\u001b[0m [23-06-06 15:20:46] Success: [xgb_os#P40] Normalizing numeric features (0.32s).\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=112791)\u001b[0m [23-06-06 15:20:46] In progress: [xgb_os#P40] 1-th Feature selection.\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=113619)\u001b[0m [23-06-06 15:20:47] In progress: [xgb_os#P03] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=113619)\u001b[0m [23-06-06 15:20:47] Success: [xgb_os#P03] Oversampling (0.08s).\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:48] Success: [xgb_os#P48] 1-th Feature selection (0.40s).\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 158 (# Cat. = 0; # Num. = 158)\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:48] In progress: [xgb_os#P48] Training.\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:50] Success: [xgb_os#P50] Training (0.44s).\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:51] In progress: [xgb_os#P61] Normalizing numeric features.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154512, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:51] Success: [xgb_os#P66] Normalizing numeric features (0.17s).\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:51] In progress: [xgb_os#P61] 1-th Feature selection.\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:51] In progress: [xgb_os#P61] Oversampling.\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:52] Success: [xgb_os#P61] Oversampling (0.04s).\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:53] Success: [xgb_os#P72] 1-th Feature selection (1.06s).\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 158 (# Cat. = 0; # Num. = 158)\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154637, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:53] In progress: [xgb_os#P72] Training.\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154551, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:54] Success: [xgb_os#P76] Training (0.37s).\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:53] In progress: [xgb_os#P80] Normalizing numeric features.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:53] Success: [xgb_os#P80] Normalizing numeric features (0.21s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1153452, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:53] In progress: [xgb_os#P80] 1-th Feature selection.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:54] In progress: [xgb_os#P78] Oversampling.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:54] Success: [xgb_os#P75] Oversampling (0.03s).\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:54] Success: [xgb_os#P78] 1-th Feature selection (1.04s).\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 2791 (# Cat. = 0; # Num. = 2791)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154322, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 174 (# Cat. = 0; # Num. = 174)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1154638, ip=192.168.1.27)\u001b[0m [23-06-06 15:20:54] In progress: [xgb_os#P75] Training.\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    threshold=1e-5\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_DUMMY),\n",
    "        oversample=False,\n",
    "        select=None,\n",
    "        name='dummy'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    )\n",
    "]\n",
    "\n",
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress.pkl')\n",
    "par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', 'stress')\n",
    "\n",
    "if os.path.isdir(par_dir):\n",
    "    # Get a list of all the files in the folder\n",
    "    files = os.listdir(par_dir)\n",
    "\n",
    "    # Delete all the files in the folder\n",
    "    for file in files:\n",
    "        if file !='.ipynb_checkpoints':\n",
    "            os.remove(os.path.join(par_dir, file))\n",
    "os.makedirs(par_dir, exist_ok=True)\n",
    "\n",
    "#with on_ray(num_cpus=6):\n",
    "with on_ray():\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):       \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        #The following code is for excluding 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        \n",
    "        \n",
    "        ###########################################\n",
    "        #The following code is for similar-user model\n",
    "        ###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "        ###########################################\n",
    "        \n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] \n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "        feat_final = pd.concat([feat_current_ESM, feat_today_ESM, feat_today_sensor],axis=1)\n",
    "        X = feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups,\n",
    "            path=par_dir,\n",
    "            categories=cats,\n",
    "            normalize=True,\n",
    "            split='logo',\n",
    "            split_params= {'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_prob,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            ) if not is_same_y else np.nan\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_prob[:, 1], average=None\n",
    "        ) if not is_same_y else np.nan\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_prob[:, i], pos_label=c, average=None\n",
    "            ) \n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_prob, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_prob=y_prob[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>split</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_class_ratio</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>...</th>\n",
       "      <th>train_pre_1</th>\n",
       "      <th>train_rec_1</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_roauc</th>\n",
       "      <th>train_prauc_0</th>\n",
       "      <th>train_prauc_ref_0</th>\n",
       "      <th>train_prauc_1</th>\n",
       "      <th>train_prauc_ref_1</th>\n",
       "      <th>train_log_loss</th>\n",
       "      <th>train_brier_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>rf_ns</td>\n",
       "      <td>P42</td>\n",
       "      <td>160</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>44</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950192</td>\n",
       "      <td>0.945843</td>\n",
       "      <td>0.948012</td>\n",
       "      <td>0.991693</td>\n",
       "      <td>0.991419</td>\n",
       "      <td>0.485681</td>\n",
       "      <td>0.992371</td>\n",
       "      <td>0.514319</td>\n",
       "      <td>0.181549</td>\n",
       "      <td>0.045938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>rf_os</td>\n",
       "      <td>P49</td>\n",
       "      <td>161</td>\n",
       "      <td>79</td>\n",
       "      <td>56</td>\n",
       "      <td>23</td>\n",
       "      <td>2.434783</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>0.993191</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.993106</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.172201</td>\n",
       "      <td>0.042920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>P55</td>\n",
       "      <td>167</td>\n",
       "      <td>56</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929742</td>\n",
       "      <td>0.895489</td>\n",
       "      <td>0.912294</td>\n",
       "      <td>0.972424</td>\n",
       "      <td>0.970776</td>\n",
       "      <td>0.481077</td>\n",
       "      <td>0.975536</td>\n",
       "      <td>0.518923</td>\n",
       "      <td>0.253849</td>\n",
       "      <td>0.071666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_os</td>\n",
       "      <td>P52</td>\n",
       "      <td>160</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>1.529412</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936882</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.928760</td>\n",
       "      <td>0.978255</td>\n",
       "      <td>0.975030</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.980688</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.056564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_os</td>\n",
       "      <td>P69</td>\n",
       "      <td>164</td>\n",
       "      <td>76</td>\n",
       "      <td>30</td>\n",
       "      <td>46</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931255</td>\n",
       "      <td>0.889992</td>\n",
       "      <td>0.910156</td>\n",
       "      <td>0.974382</td>\n",
       "      <td>0.974088</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.976312</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.240402</td>\n",
       "      <td>0.067660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label     alg split  n_feature  test_inst  test_inst_0  test_inst_1  \\\n",
       "0  stress   rf_ns   P42        160         70           26           44   \n",
       "1  stress   rf_os   P49        161         79           56           23   \n",
       "2  stress  xgb_ns   P55        167         56           31           25   \n",
       "3  stress  xgb_os   P52        160         43           26           17   \n",
       "4  stress  xgb_os   P69        164         76           30           46   \n",
       "\n",
       "   test_class_ratio  test_true_0_pred_0  test_true_0_pred_1  ...  train_pre_1  \\\n",
       "0          0.590909                  10                  16  ...     0.950192   \n",
       "1          2.434783                  49                   7  ...     0.949700   \n",
       "2          1.240000                  27                   4  ...     0.929742   \n",
       "3          1.529412                  24                   2  ...     0.936882   \n",
       "4          0.652174                  24                   6  ...     0.931255   \n",
       "\n",
       "   train_rec_1  train_f1_1  train_roauc  train_prauc_0  train_prauc_ref_0  \\\n",
       "0     0.945843    0.948012     0.991693       0.991419           0.485681   \n",
       "1     0.949700    0.949700     0.992981       0.993191           0.500000   \n",
       "2     0.895489    0.912294     0.972424       0.970776           0.481077   \n",
       "3     0.920777    0.928760     0.978255       0.975030           0.500000   \n",
       "4     0.889992    0.910156     0.974382       0.974088           0.500000   \n",
       "\n",
       "   train_prauc_1  train_prauc_ref_1  train_log_loss  train_brier_loss  \n",
       "0       0.992371           0.514319        0.181549          0.045938  \n",
       "1       0.993106           0.500000        0.172201          0.042920  \n",
       "2       0.975536           0.518923        0.253849          0.071666  \n",
       "3       0.980688           0.500000        0.203400          0.056564  \n",
       "4       0.976312           0.500000        0.240402          0.067660  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "#for l in ['valence', 'arousal', 'disturbance', 'stress']:\n",
    "for l in [ 'stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        if f == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        model, pid = f[:f.index('.pkl')].split('#')\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "        X, y = res.X_test, res.y_test\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_test = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        X, y = res.X_train, res.y_train\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_train = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        RESULTS_EVAL.append({\n",
    "            'label': l,\n",
    "            'alg': model,\n",
    "            'split': pid,\n",
    "            'n_feature': len(X.columns),\n",
    "            **{\n",
    "                f'test_{k}': v for k, v in ev_test.items()\n",
    "            },\n",
    "            **{\n",
    "                f'train_{k}': v for k, v in ev_train.items()\n",
    "            }\n",
    "        })\n",
    "    \n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'alg', 'split', 'n_feature', 'test_inst', 'test_inst_0',\n",
       "       'test_inst_1', 'test_class_ratio', 'test_true_0_pred_0',\n",
       "       'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1',\n",
       "       'test_acc', 'test_bac', 'test_gmean', 'test_mcc', 'test_pre_macro',\n",
       "       'test_rec_macro', 'test_f1_macro', 'test_pre_0', 'test_rec_0',\n",
       "       'test_f1_0', 'test_pre_1', 'test_rec_1', 'test_f1_1', 'test_roauc',\n",
       "       'test_prauc_0', 'test_prauc_ref_0', 'test_prauc_1', 'test_prauc_ref_1',\n",
       "       'test_log_loss', 'test_brier_loss', 'train_inst', 'train_inst_0',\n",
       "       'train_inst_1', 'train_class_ratio', 'train_true_0_pred_0',\n",
       "       'train_true_0_pred_1', 'train_true_1_pred_0', 'train_true_1_pred_1',\n",
       "       'train_acc', 'train_bac', 'train_gmean', 'train_mcc', 'train_pre_macro',\n",
       "       'train_rec_macro', 'train_f1_macro', 'train_pre_0', 'train_rec_0',\n",
       "       'train_f1_0', 'train_pre_1', 'train_rec_1', 'train_f1_1', 'train_roauc',\n",
       "       'train_prauc_0', 'train_prauc_ref_0', 'train_prauc_1',\n",
       "       'train_prauc_ref_1', 'train_log_loss', 'train_brier_loss'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_EVAL.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_xgbos = RESULTS_EVAL[RESULTS_EVAL['alg']=='xgb_os']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_performance = RESULTS_xgbos['test_f1_macro'] >= 0.761\n",
    "RESULTS_xgbos_high_performance = RESULTS_xgbos[mask_performance]\n",
    "RESULTS_xgbos_low_performance = RESULTS_xgbos[~mask_performance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_bac</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_roauc</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>test_true_1_pred_0</th>\n",
       "      <th>test_true_1_pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P61</td>\n",
       "      <td>73</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.639803</td>\n",
       "      <td>0.811355</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P78</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P60</td>\n",
       "      <td>61</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>0.712243</td>\n",
       "      <td>0.715618</td>\n",
       "      <td>0.772311</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>P01</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>0.751918</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.874680</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>P45</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>0.670034</td>\n",
       "      <td>0.649231</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>P53</td>\n",
       "      <td>76</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>0.735192</td>\n",
       "      <td>0.735192</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>P42</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>44</td>\n",
       "      <td>0.750874</td>\n",
       "      <td>0.759780</td>\n",
       "      <td>0.905157</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>P39</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.808642</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>P57</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>P75</td>\n",
       "      <td>73</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>0.639037</td>\n",
       "      <td>0.643902</td>\n",
       "      <td>0.851604</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>P51</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>0.695804</td>\n",
       "      <td>0.675644</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>P35</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>0.685897</td>\n",
       "      <td>0.691249</td>\n",
       "      <td>0.904915</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>P55</td>\n",
       "      <td>56</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>0.659355</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>P05</td>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.833045</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>P28</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>P12</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.869748</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>P09</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>0.666409</td>\n",
       "      <td>0.680946</td>\n",
       "      <td>0.925697</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>P21</td>\n",
       "      <td>50</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>0.699507</td>\n",
       "      <td>0.702886</td>\n",
       "      <td>0.869458</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>P79</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.638796</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>P50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.743393</td>\n",
       "      <td>0.837366</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>P13</td>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.722059</td>\n",
       "      <td>0.893690</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    split  test_inst  test_inst_0  test_inst_1  test_bac  test_f1_macro  \\\n",
       "23    P61         73           52           21  0.628205       0.639803   \n",
       "33    P78         50            5           45  0.500000       0.490196   \n",
       "34    P60         61           38           23  0.712243       0.715618   \n",
       "35    P01         40           23           17  0.751918       0.747475   \n",
       "37    P45         38           11           27  0.670034       0.649231   \n",
       "44    P53         76           41           35  0.735192       0.735192   \n",
       "52    P42         70           26           44  0.750874       0.759780   \n",
       "83    P39         60            6           54  0.555556       0.563636   \n",
       "85    P57         57           50            7  0.735714       0.709184   \n",
       "91    P75         73           51           22  0.639037       0.643902   \n",
       "92    P51         63           11           52  0.695804       0.675644   \n",
       "114   P35         51           12           39  0.685897       0.691249   \n",
       "135   P55         56           31           25  0.659355       0.657143   \n",
       "138   P05         51           17           34  0.647059       0.647059   \n",
       "170   P28         83            2           81  0.500000       0.493902   \n",
       "171   P12         45           28           17  0.803571       0.755556   \n",
       "172   P09         55           38           17  0.666409       0.680946   \n",
       "175   P21         50           21           29  0.699507       0.702886   \n",
       "193   P79         36           18           18  0.666667       0.638796   \n",
       "200   P50         43           31           12  0.708333       0.743393   \n",
       "226   P13         54           27           27  0.740741       0.722059   \n",
       "\n",
       "     test_roauc  test_true_0_pred_0  test_true_0_pred_1  test_true_1_pred_0  \\\n",
       "23     0.811355                  48                   4                  14   \n",
       "33     0.697778                   1                   4                   9   \n",
       "34     0.772311                  31                   7                   9   \n",
       "35     0.874680                  17                   6                   4   \n",
       "37     0.781145                   7                   4                   8   \n",
       "44     0.828571                  31                  10                  10   \n",
       "52     0.905157                  16                  10                   5   \n",
       "83     0.808642                   1                   5                   3   \n",
       "85     0.878571                  45                   5                   3   \n",
       "91     0.851604                  42                   9                  12   \n",
       "92     0.846154                   6                   5                   8   \n",
       "114    0.904915                   6                   6                   5   \n",
       "135    0.747097                  26                   5                  13   \n",
       "138    0.833045                   9                   8                   8   \n",
       "170    0.796296                   0                   2                   0   \n",
       "171    0.869748                  17                  11                   0   \n",
       "172    0.925697                  35                   3                  10   \n",
       "175    0.869458                  12                   9                   5   \n",
       "193    0.731481                  17                   1                  11   \n",
       "200    0.837366                  31                   0                   7   \n",
       "226    0.893690                  13                  14                   0   \n",
       "\n",
       "     test_true_1_pred_1  \n",
       "23                    7  \n",
       "33                   36  \n",
       "34                   14  \n",
       "35                   13  \n",
       "37                   19  \n",
       "44                   25  \n",
       "52                   39  \n",
       "83                   51  \n",
       "85                    4  \n",
       "91                   10  \n",
       "92                   44  \n",
       "114                  34  \n",
       "135                  12  \n",
       "138                  26  \n",
       "170                  81  \n",
       "171                  17  \n",
       "172                   7  \n",
       "175                  24  \n",
       "193                   7  \n",
       "200                   5  \n",
       "226                  27  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_xgbos_low_performance[['split','test_inst','test_inst_0','test_inst_1','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "       'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_bac</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_roauc</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>test_true_1_pred_0</th>\n",
       "      <th>test_true_1_pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P52</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>0.765837</td>\n",
       "      <td>0.773021</td>\n",
       "      <td>0.871041</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P69</td>\n",
       "      <td>76</td>\n",
       "      <td>30</td>\n",
       "      <td>46</td>\n",
       "      <td>0.763043</td>\n",
       "      <td>0.764534</td>\n",
       "      <td>0.880072</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P33</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.928947</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P30</td>\n",
       "      <td>70</td>\n",
       "      <td>54</td>\n",
       "      <td>16</td>\n",
       "      <td>0.766204</td>\n",
       "      <td>0.772122</td>\n",
       "      <td>0.891204</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P19</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>0.763492</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P48</td>\n",
       "      <td>37</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>0.819940</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>0.930060</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P15</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>0.837931</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P08</td>\n",
       "      <td>69</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>0.883929</td>\n",
       "      <td>0.894388</td>\n",
       "      <td>0.941468</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P26</td>\n",
       "      <td>76</td>\n",
       "      <td>47</td>\n",
       "      <td>29</td>\n",
       "      <td>0.836757</td>\n",
       "      <td>0.843299</td>\n",
       "      <td>0.933602</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>P40</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>0.971698</td>\n",
       "      <td>0.849073</td>\n",
       "      <td>0.983491</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>P31</td>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.867168</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>P72</td>\n",
       "      <td>65</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>0.783181</td>\n",
       "      <td>0.800858</td>\n",
       "      <td>0.889016</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>P67</td>\n",
       "      <td>65</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>0.776888</td>\n",
       "      <td>0.776888</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>P70</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.809082</td>\n",
       "      <td>0.966783</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>P02</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>0.840116</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>P80</td>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>0.808608</td>\n",
       "      <td>0.807114</td>\n",
       "      <td>0.902930</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>P49</td>\n",
       "      <td>79</td>\n",
       "      <td>56</td>\n",
       "      <td>23</td>\n",
       "      <td>0.755823</td>\n",
       "      <td>0.778711</td>\n",
       "      <td>0.883929</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>P03</td>\n",
       "      <td>44</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.972632</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>P76</td>\n",
       "      <td>65</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>0.780556</td>\n",
       "      <td>0.792212</td>\n",
       "      <td>0.916111</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>P06</td>\n",
       "      <td>49</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.765754</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>P47</td>\n",
       "      <td>77</td>\n",
       "      <td>26</td>\n",
       "      <td>51</td>\n",
       "      <td>0.824661</td>\n",
       "      <td>0.796404</td>\n",
       "      <td>0.937029</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>P32</td>\n",
       "      <td>52</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.784557</td>\n",
       "      <td>0.934454</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>P23</td>\n",
       "      <td>49</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>0.846395</td>\n",
       "      <td>0.920543</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>P66</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.924324</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>P77</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>0.885093</td>\n",
       "      <td>0.885833</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>P10</td>\n",
       "      <td>47</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>0.812963</td>\n",
       "      <td>0.819923</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    split  test_inst  test_inst_0  test_inst_1  test_bac  test_f1_macro  \\\n",
       "3     P52         43           26           17  0.765837       0.773021   \n",
       "4     P69         76           30           46  0.763043       0.764534   \n",
       "8     P33         39           20           19  0.797368       0.793651   \n",
       "14    P30         70           54           16  0.766204       0.772122   \n",
       "15    P19         52           45            7  0.763492       0.780220   \n",
       "16    P48         37           16           21  0.819940       0.827640   \n",
       "24    P15         39           29           10  0.800000       0.842742   \n",
       "28    P08         69           21           48  0.883929       0.894388   \n",
       "31    P26         76           47           29  0.836757       0.843299   \n",
       "38    P40         57            4           53  0.971698       0.849073   \n",
       "50    P31         53           25           28  0.866429       0.867168   \n",
       "89    P72         65           46           19  0.783181       0.800858   \n",
       "106   P67         65           46           19  0.776888       0.776888   \n",
       "107   P70         37           11           26  0.772727       0.809082   \n",
       "112   P02         51            8           43  0.840116       0.823529   \n",
       "121   P80         47           26           21  0.808608       0.807114   \n",
       "122   P49         79           56           23  0.755823       0.778711   \n",
       "141   P03         44           25           19  0.947368       0.952991   \n",
       "153   P76         65           20           45  0.780556       0.792212   \n",
       "165   P06         49           21           28  0.761905       0.765754   \n",
       "167   P47         77           26           51  0.824661       0.796404   \n",
       "188   P32         52           17           35  0.842857       0.784557   \n",
       "195   P23         49           43            6  0.821705       0.846395   \n",
       "211   P66         52           15           37  0.832432       0.819444   \n",
       "228   P77         44           21           23  0.885093       0.885833   \n",
       "232   P10         47           27           20  0.812963       0.819923   \n",
       "\n",
       "     test_roauc  test_true_0_pred_0  test_true_0_pred_1  test_true_1_pred_0  \\\n",
       "3      0.871041                  23                   3                   6   \n",
       "4      0.880072                  21                   9                   8   \n",
       "8      0.928947                  14                   6                   2   \n",
       "14     0.891204                  49                   5                   6   \n",
       "15     0.860317                  43                   2                   3   \n",
       "16     0.930060                  11                   5                   1   \n",
       "24     0.837931                  29                   0                   4   \n",
       "28     0.941468                  17                   4                   2   \n",
       "31     0.933602                  43                   4                   7   \n",
       "38     0.983491                   4                   0                   3   \n",
       "50     0.905000                  21                   4                   3   \n",
       "89     0.889016                  43                   3                   7   \n",
       "106    0.881579                  40                   6                   6   \n",
       "107    0.966783                   6                   5                   0   \n",
       "112    0.843023                   6                   2                   3   \n",
       "121    0.902930                  21                   5                   4   \n",
       "122    0.883929                  53                   3                  10   \n",
       "141    0.972632                  25                   0                   2   \n",
       "153    0.916111                  13                   7                   4   \n",
       "165    0.913265                  14                   7                   4   \n",
       "167    0.937029                  23                   3                  12   \n",
       "188    0.934454                  17                   0                  11   \n",
       "195    0.920543                  42                   1                   2   \n",
       "211    0.924324                  12                   3                   5   \n",
       "228    0.927536                  18                   3                   2   \n",
       "232    0.951852                  25                   2                   6   \n",
       "\n",
       "     test_true_1_pred_1  \n",
       "3                    11  \n",
       "4                    38  \n",
       "8                    17  \n",
       "14                   10  \n",
       "15                    4  \n",
       "16                   20  \n",
       "24                    6  \n",
       "28                   46  \n",
       "31                   22  \n",
       "38                   50  \n",
       "50                   25  \n",
       "89                   12  \n",
       "106                  13  \n",
       "107                  26  \n",
       "112                  40  \n",
       "121                  17  \n",
       "122                  13  \n",
       "141                  17  \n",
       "153                  41  \n",
       "165                  24  \n",
       "167                  39  \n",
       "188                  24  \n",
       "195                   4  \n",
       "211                  32  \n",
       "228                  21  \n",
       "232                  14  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_xgbos_high_performance[['split','test_inst','test_inst_0','test_inst_1','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "       'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code is for cluster analysis\n",
    "# pcode_to_cluster = dict(zip(similar_user['pcode'], similar_user['cluster']))\n",
    "# RESULTS_EVAL['cluster'] = RESULTS_EVAL['split'].map(pcode_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>metric</th>\n",
       "      <th>n</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>value_count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>med</th>\n",
       "      <th>range</th>\n",
       "      <th>conf.</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>split</td>\n",
       "      <td>47</td>\n",
       "      <td>47.0</td>\n",
       "      <td>P77:1, P13:1, P60:1, P80:1, P61:1, P23:1, P55:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>n_feature</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131177.0</td>\n",
       "      <td>2791.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2791.0</td>\n",
       "      <td>(2791, 2791)</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>55.723404</td>\n",
       "      <td>13.076202</td>\n",
       "      <td>52.0</td>\n",
       "      <td>(36, 83)</td>\n",
       "      <td>(51.88408763344431, 59.56272087719398)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst_0</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>26.893617</td>\n",
       "      <td>14.547126</td>\n",
       "      <td>26.0</td>\n",
       "      <td>(2, 56)</td>\n",
       "      <td>(22.622420860143833, 31.16481318240936)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst_1</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>28.829787</td>\n",
       "      <td>15.207720</td>\n",
       "      <td>25.0</td>\n",
       "      <td>(6, 81)</td>\n",
       "      <td>(24.364633393179524, 33.29494107490558)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label    alg       metric   n  cardinality  \\\n",
       "0  stress  dummy        split  47         47.0   \n",
       "1  stress  dummy    n_feature  47          NaN   \n",
       "2  stress  dummy    test_inst  47          NaN   \n",
       "3  stress  dummy  test_inst_0  47          NaN   \n",
       "4  stress  dummy  test_inst_1  47          NaN   \n",
       "\n",
       "                                         value_count       sum         mean  \\\n",
       "0  P77:1, P13:1, P60:1, P80:1, P61:1, P23:1, P55:...       NaN          NaN   \n",
       "1                                                NaN  131177.0  2791.000000   \n",
       "2                                                NaN    2619.0    55.723404   \n",
       "3                                                NaN    1264.0    26.893617   \n",
       "4                                                NaN    1355.0    28.829787   \n",
       "\n",
       "          SD     med         range                                    conf.  \\\n",
       "0        NaN     NaN           NaN                                      NaN   \n",
       "1   0.000000  2791.0  (2791, 2791)                               (nan, nan)   \n",
       "2  13.076202    52.0      (36, 83)   (51.88408763344431, 59.56272087719398)   \n",
       "3  14.547126    26.0       (2, 56)  (22.622420860143833, 31.16481318240936)   \n",
       "4  15.207720    25.0       (6, 81)  (24.364633393179524, 33.29494107490558)   \n",
       "\n",
       "   nan_count  \n",
       "0        NaN  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "#    ['label', 'alg', 'cluster']\n",
    "     ['label', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                alg=row.alg,\n",
    "#                 cluster = row.cluster,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">mean_sd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_bac</th>\n",
       "      <th>test_f1_0</th>\n",
       "      <th>test_f1_1</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_roauc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">stress</th>\n",
       "      <th>dummy</th>\n",
       "      <td>2791.0 (0.0)</td>\n",
       "      <td>0.5 (0.0)</td>\n",
       "      <td>0.0 (0.0)</td>\n",
       "      <td>0.652 (0.2)</td>\n",
       "      <td>0.326 (0.1)</td>\n",
       "      <td>0.5 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_ns</th>\n",
       "      <td>163.766 (6.575)</td>\n",
       "      <td>0.728 (0.091)</td>\n",
       "      <td>0.716 (0.194)</td>\n",
       "      <td>0.743 (0.161)</td>\n",
       "      <td>0.729 (0.091)</td>\n",
       "      <td>0.839 (0.077)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_os</th>\n",
       "      <td>163.766 (6.575)</td>\n",
       "      <td>0.725 (0.085)</td>\n",
       "      <td>0.727 (0.186)</td>\n",
       "      <td>0.727 (0.17)</td>\n",
       "      <td>0.727 (0.087)</td>\n",
       "      <td>0.84 (0.074)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_ns</th>\n",
       "      <td>163.766 (6.575)</td>\n",
       "      <td>0.757 (0.081)</td>\n",
       "      <td>0.74 (0.174)</td>\n",
       "      <td>0.758 (0.138)</td>\n",
       "      <td>0.749 (0.087)</td>\n",
       "      <td>0.874 (0.057)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_os</th>\n",
       "      <td>163.766 (6.575)</td>\n",
       "      <td>0.769 (0.095)</td>\n",
       "      <td>0.762 (0.184)</td>\n",
       "      <td>0.776 (0.139)</td>\n",
       "      <td>0.769 (0.094)</td>\n",
       "      <td>0.879 (0.066)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean_sd                                               \\\n",
       "metric               n_feature       test_bac      test_f1_0      test_f1_1   \n",
       "label  alg                                                                    \n",
       "stress dummy      2791.0 (0.0)      0.5 (0.0)      0.0 (0.0)    0.652 (0.2)   \n",
       "       rf_ns   163.766 (6.575)  0.728 (0.091)  0.716 (0.194)  0.743 (0.161)   \n",
       "       rf_os   163.766 (6.575)  0.725 (0.085)  0.727 (0.186)   0.727 (0.17)   \n",
       "       xgb_ns  163.766 (6.575)  0.757 (0.081)   0.74 (0.174)  0.758 (0.138)   \n",
       "       xgb_os  163.766 (6.575)  0.769 (0.095)  0.762 (0.184)  0.776 (0.139)   \n",
       "\n",
       "                                             \n",
       "metric         test_f1_macro     test_roauc  \n",
       "label  alg                                   \n",
       "stress dummy     0.326 (0.1)      0.5 (0.0)  \n",
       "       rf_ns   0.729 (0.091)  0.839 (0.077)  \n",
       "       rf_os   0.727 (0.087)   0.84 (0.074)  \n",
       "       xgb_ns  0.749 (0.087)  0.874 (0.057)  \n",
       "       xgb_os  0.769 (0.094)  0.879 (0.066)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUB_SUMMARY_EVAL = SUMMARY_EVAL[SUMMARY_EVAL['alg']=='xgb_os'].loc[\n",
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "#        ['n_feature', 'train_class_ratio', 'train_inst_0', 'train_inst_1', 'test_inst_0', 'test_inst_1', 'test_bac', 'test_f1_0' ,'test_f1_1', 'test_f1_macro', 'train_f1_0' ,'train_f1_1', 'train_f1_macro','test_roauc']\n",
    "        ['n_feature', 'test_bac', 'test_f1_0' ,'test_f1_1', 'test_f1_macro','test_roauc']\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep='')\n",
    ").pivot(\n",
    "#    index=['label', 'alg', 'cluster'], columns=['metric'], values=['mean_sd']\n",
    "    index=['label', 'alg'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "# for l in ['valence', 'arousal', 'disturbance', 'stress']:\n",
    "for l in ['stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        if f!='.ipynb_checkpoints':\n",
    "            res = load(os.path.join(dir_l, f))\n",
    "\n",
    "            f_norm = f[:f.index('.pkl')]\n",
    "            alg = f_norm[:f.rindex('#')]\n",
    "\n",
    "            feat_imp = feature_importance(res.estimator)\n",
    "            if not feat_imp:\n",
    "                continue\n",
    "\n",
    "            names, importance = feat_imp\n",
    "            new_names = []\n",
    "            for n in names:\n",
    "                for c in res.categories:\n",
    "                    n = n.replace(f'{c}_', f'{c}=')\n",
    "                new_names.append(n)\n",
    "\n",
    "            d = pd.DataFrame(\n",
    "                importance.reshape(1, -1),\n",
    "                columns=new_names\n",
    "            )\n",
    "            IMPORTANCE_EVAL[(l, alg)].append(d)\n",
    "        \n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for (l, alg), v in IMPORTANCE_EVAL.items():\n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        alg=alg\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attaching packages  tidyverse 1.3.2 \n",
      " ggplot2 3.4.0       purrr   1.0.0 \n",
      " tibble  3.1.8       dplyr   1.0.10\n",
      " tidyr   1.2.1       stringr 1.5.0 \n",
      " readr   2.1.3       forcats 0.5.2 \n",
      " Conflicts  tidyverse_conflicts() \n",
      " dplyr::filter() masks stats::filter()\n",
      " dplyr::lag()    masks stats::lag()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: sysfonts\n",
      "\n",
      "R[write to console]: Loading required package: showtextdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(tidyverse)\n",
    "library(ggforce)\n",
    "library(ggpubr)\n",
    "library(showtext)\n",
    "library(rmcorr)\n",
    "library(patchwork)\n",
    "\n",
    "font_add_google(\n",
    "    name='Source Serif Pro',\n",
    "    family='ssp',\n",
    "    db_cache=FALSE\n",
    ")\n",
    "\n",
    "showtext_auto()\n",
    "\n",
    "THEME_DEFAULT <- theme_bw(\n",
    "    base_size=10,\n",
    "    base_family='ssp',\n",
    ") + theme(\n",
    "        axis.title.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.title.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.text.x=element_text(colour='grey20', size=10),\n",
    "        axis.text.y=element_text(colour='grey20', size=10),\n",
    "        strip.text.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        strip.text.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.background=element_blank(),\n",
    "        legend.title=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.text=element_text(colour='grey20', size=10),\n",
    "        legend.position='top',\n",
    "        legend.box.spacing= unit(0, 'cm'),\n",
    "        plot.subtitle=element_text(colour='grey20', size=10, hjust=.5),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panyu/miniconda3/envs/sci-data/lib/python3.9/site-packages/rpy2/robjects/pandas2ri.py:65: UserWarning: Error while trying to convert the column \"feature\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'numpy.str_'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 32 -u cm\n",
    "\n",
    "plots <- list()\n",
    "\n",
    "#for (l in c('valence', 'arousal', 'stress', 'disturbance')) {\n",
    "for (l in c( 'stress')) {\n",
    "    data <- IMPORTANCE_SUMMARY %>% filter(\n",
    "        (label == l)\n",
    "    )\n",
    "\n",
    "    p_label <- ggplot() + geom_text(\n",
    "        aes(x=.5, y=.5),\n",
    "        label=str_to_title(l), \n",
    "        family='ssp', \n",
    "        fontface='bold',\n",
    "        size=4\n",
    "    ) + theme_void()\n",
    "\n",
    "    p_rf <- ggplot(\n",
    "        data %>% filter(alg == 'rf_os') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1, vjust=.5),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='Random Forest'\n",
    "    )\n",
    "    \n",
    "    p_xgb <- ggplot(\n",
    "        data %>% filter(alg == 'xgb_os') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1, vjust=.5),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='XGBoost'\n",
    "    )\n",
    "    \n",
    "    plots[[paste(l, 'label', sep='_')]] <- p_label\n",
    "    plots[[paste(l, 'rf', sep='_')]] <- p_rf\n",
    "    plots[[paste(l, 'xgb', sep='_')]] <- p_xgb\n",
    "}\n",
    "\n",
    "#p <- plots$arousal_label + plots$valence_label\n",
    "#p <- p / (plots$arousal_rf | plots$arousal_xgb | plots$valence_rf | plots$valence_xgb)\n",
    "#p <- p / (plots$stress_label + plots$disturbance_label)\n",
    "#p <- p / (plots$stress_rf | plots$stress_xgb | plots$disturbance_rf | plots$disturbance_xgb)\n",
    "p <- plots$stress_label \n",
    "p <- p / (plots$stress_rf | plots$stress_xgb)\n",
    "\n",
    "p <- p + plot_layout(\n",
    "    heights=c(1.1, 10, 1.1, 10)\n",
    ")\n",
    "\n",
    "ggsave(paste('./fig/imp.pdf'), plot=p, width=26, height=32, unit='cm', device=cairo_pdf)\n",
    "print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
